<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2026-02-10">

<title>The Residual Stream</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-ed7b9cc99f06e9b47f712dd9fcc9dedf.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="floating nav-fixed slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Andrew Stephen Fletcher</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/andrewstephenfletcher" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#transformers" id="toc-transformers" class="nav-link active" data-scroll-target="#transformers">Transformers</a></li>
  <li><a href="#why-residuals" id="toc-why-residuals" class="nav-link" data-scroll-target="#why-residuals">Why residuals?</a></li>
  <li><a href="#the-residual-stream-in-mechanistic-interpretability" id="toc-the-residual-stream-in-mechanistic-interpretability" class="nav-link" data-scroll-target="#the-residual-stream-in-mechanistic-interpretability">The Residual Stream in Mechanistic Interpretability</a>
  <ul class="collapse">
  <li><a href="#logit-lens" id="toc-logit-lens" class="nav-link" data-scroll-target="#logit-lens">Logit Lens</a></li>
  <li><a href="#linear-probes" id="toc-linear-probes" class="nav-link" data-scroll-target="#linear-probes">Linear Probes</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The Residual Stream</h1>
  <div class="quarto-categories">
    <div class="quarto-category">mechanistic interpretability</div>
    <div class="quarto-category">residual stream</div>
    <div class="quarto-category">linear probes</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 10, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>This post discusses the residual stream in transformer models and builds towards using it for two examples of mechanistic interpretability: Logit Lens <span class="citation" data-cites="Logit-Lens">(<a href="#ref-Logit-Lens" role="doc-biblioref">nostalgebraist 2020</a>)</span> and Linear Probes <span class="citation" data-cites="Linear-Probes">(<a href="#ref-Linear-Probes" role="doc-biblioref">Alain and Bengio 2018</a>)</span>.</p>
<section id="transformers" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="transformers">Transformers</h2>
<p>Much of the description of a transformer here comes straight from the excellent “A Mathematical Framework for Transformer Circuits” <span class="citation" data-cites="Mathematical-Framework-for-Transformer-Circuits">(<a href="#ref-Mathematical-Framework-for-Transformer-Circuits" role="doc-biblioref">Elhage et al. 2021</a>)</span>.</p>
<p>The transformer is made of a few crucial components:</p>
<ul>
<li>Token embedding, <span class="math inline">\(W_{E}\)</span></li>
<li>Residual Stream, <span class="math inline">\(x_{i}\)</span></li>
<li>Attention layer, <span class="math inline">\(Attn(x_{i}) = \sum_{h \in H} h(x_{i})\)</span></li>
<li>MLP Layer, <span class="math inline">\(MLP(x_{i})\)</span></li>
<li>Unembedding, <span class="math inline">\(W_{U}\)</span></li>
</ul>
<p>I will give a high level overview of what some of these elements do by considering the input sequence,</p>
<div style="text-align: center; font-style: italic; font-size: 1.2em; margin: 20px 0;">
<p>“The capital of the country containing Manchester is”</p>
</div>
<p>We consider the final token “is” as this is the token from which the model will predict the first unseen token, which we hope will be the correct answer London.</p>
<p>The first step is to use the Token embedding, <span class="math inline">\(W_{E} \in \mathbb{R}^{d_{model} \times d_{vocab}}\)</span>, which acts as a look-up table from the token “is”, which we represent initially by the token index <span class="math inline">\(t \in \mathbb{R}^{d_{vocab}}\)</span>, to the <span class="math inline">\(d_{model}\)</span> vector that represents it,</p>
<p><span id="eq-embedding"><span class="math display">\[
x_{0} = W_{E} t
\tag{1}\]</span></span></p>
<p>This is the source of the residual stream, each process in the model adds <em>something</em> to this initial vector <span class="math inline">\(x_{0}\)</span><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. The vector <span class="math inline">\(x_{0}\)</span> starts as an isolated embedding of the token “is” but finishes as a representation capable of predicting the next token in the sequence.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;There has been some progress towards not using the simple additive residual stream. One such example are Hyper-Connections <span class="citation" data-cites="Hyper-Connections">(<a href="#ref-Hyper-Connections" role="doc-biblioref">Zhu et al. 2025</a>)</span>, which allow the strength of connections between layers to be learnt during training.</p></div></div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/residual-stream.svg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>High level diagram of the residual stream</figcaption>
</figure>
</div>
<p>Most commonly a <em>layer</em><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> of a transformer refers to the combination of an Attention layer and an MLP layer but they add to the residual stream sequentially,</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;<span class="citation" data-cites="Mathematical-Framework-for-Transformer-Circuits">(<a href="#ref-Mathematical-Framework-for-Transformer-Circuits" role="doc-biblioref">Elhage et al. 2021</a>)</span> also refers to this as a “residual block”.</p></div></div><p><span id="eq-layer"><span class="math display">\[
x_{n+1} = x_{n} + Attn(x_{n}) + MLP(x_{n} + Attn(x_{n}))
\tag{2}\]</span></span></p>
<p>Attention heads move information from the residual streams of other tokens in the sequence. For example in our example if we are going to answer the question,</p>
<div style="text-align: center; font-style: italic; font-size: 1.2em; margin: 20px 0;">
<p>“The capital of the country containing Manchester is”</p>
</div>
<p>our final representation <span class="math inline">\(x_{-1}\)</span> is going to need to capture the multi-hop reasoning that the <code>country</code> containing <code>Manchester</code> is the United Kingdom and hence the <code>capital</code> is London. Clearly to answer this we need to move information from these other tokens.</p>
<p>The model must also know these geographical facts, research suggests that this is captured <em>somewhere</em> within the MLP layers <span class="citation" data-cites="Fact-Finding">(<a href="#ref-Fact-Finding" role="doc-biblioref">Nanda et al. 2024</a>)</span>.</p>
</section>
<section id="why-residuals" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="why-residuals">Why residuals?</h2>
<p>There are a few reasons that this additive approach is favoured but their inclusion is likely a legacy of the Residual Neural Network (ResNet) <span class="citation" data-cites="ResNet">(<a href="#ref-ResNet" role="doc-biblioref">He et al. 2015</a>)</span>. This paper suggested a residual stream formulation,</p>
<p><span id="eq-ResNet-formulation"><span class="math display">\[
x + f(x)
\tag{3}\]</span></span></p>
<p>which has since also become known as a skip connection. The layers of the network return a “residual mapping” <span class="math inline">\(f(x)\)</span> which is added to the main input. From their paper the primary motivation was to combat <em>the degradation problem</em>, where deeper networks have higher training and test error. This should be slightly surprising, we might expect a model with 20 layers can be replicated within a model of 50 layers - if we set the first 20 layers to match the smaller model and the remaining 30 layers to be the identity.</p>
<p>It turns out that for networks without a residual stream the correlation between gradients decays exponentially <span class="citation" data-cites="shattered-gradients">(<a href="#ref-shattered-gradients" role="doc-biblioref">Balduzzi et al. 2018</a>)</span>, that is to say at a point <span class="math inline">\(x\)</span> the gradient <span class="math inline">\(\nabla f(x)\)</span> might be almost random compared to a near by point <span class="math inline">\(x + \delta\)</span> with gradient <span class="math inline">\(\nabla f(x + \delta)\)</span>. If the gradients are almost random gradient descent is too, we set off down the slope only to immediately find it increase again. In contrast, with skip connections the correlation between gradients decays sub-linearly.</p>
<p>A subtly different additional benefit to using a residual stream is that we also address <em>the vanishing gradient problem</em>, which is where backpropagation in a deep network will involve many applications of the chain rule such that if any of the gradients are small the total update to the weights will vanish.</p>
<p>Following this 3Blue1Brown explainer <span class="citation" data-cites="3b1b-backpropagation">(<a href="#ref-3b1b-backpropagation" role="doc-biblioref">Sanderson 2017</a>)</span> we consider a very simple network with single neuron layers, connected in sequence. Let’s say every layer consists of,</p>
<p><span id="eq-simple-network"><span class="math display">\[
f(x_{i}) = \sigma(W x_{i}  + b)
\tag{4}\]</span></span></p>
<p>The <em>sigmoid function</em><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, <span class="math inline">\(\sigma\)</span>, is defined as,</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;ReLU and GELU activation functions are designed to also address the vanishing gradient problem.</p></div></div><p><span id="eq-sigmoid"><span class="math display">\[
\sigma(z) = \frac{e^z}{1+e^z}
\tag{5}\]</span></span></p>
<p>Its derivative, <span class="math inline">\(\sigma'(z) = \sigma(z)(1 - \sigma(z))\)</span>, is bounded within the interval <span class="math inline">\((0, 0.25]\)</span>. Hence, when we calculate the gradient of the loss, <span class="math inline">\(\nabla \mathcal{L}\)</span>, in backpropagation this term will reduce the resulting value by at least a factor of 4 each layer. As we update the weights according to <span class="math inline">\(-\eta \nabla \mathcal{L}\)</span> when the gradient vanishes we stop learning.</p>
<p>With a residual stream there will always be a non-zero gradient,</p>
<p><span id="eq-residual"><span class="math display">\[
x_{i+1} = x_{i} + f(x_{i})
\tag{6}\]</span></span></p>
<p><span id="eq-residual-gradient"><span class="math display">\[
\frac{\partial(x_{i+1})}{\partial x_{i}} = 1 + f'(x_{i})
\tag{7}\]</span></span></p>
<p>The constant term <span class="math inline">\(1\)</span> means even with repeated applications of the chain rule it is always possible to trace through the network and update the weights of earlier layers.</p>
</section>
<section id="the-residual-stream-in-mechanistic-interpretability" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-residual-stream-in-mechanistic-interpretability">The Residual Stream in Mechanistic Interpretability</h2>
<p>With this consistent <span class="math inline">\(d_{model}\)</span> dimension vector flowing through the model we have an obvious place to inspect the model’s <em>thoughts</em>. As each layer refines the representation of the “is” token in preparation for predicting the next we can explore two ways to interpret this process:</p>
<ul>
<li>Logit Lens <span class="citation" data-cites="Logit-Lens">(<a href="#ref-Logit-Lens" role="doc-biblioref">nostalgebraist 2020</a>)</span></li>
<li>Linear Probes <span class="citation" data-cites="Linear-Probes">(<a href="#ref-Linear-Probes" role="doc-biblioref">Alain and Bengio 2018</a>)</span></li>
</ul>
<p>In the rest of this post I have some experiments that use these techniques to illustrate their value.</p>
<section id="logit-lens" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="logit-lens">Logit Lens</h3>
<p>The idea beyhind Logit Lens is simple, after the final layer, <span class="math inline">\(x_{-1}\)</span>, of the transformer we apply an unembedding to get logits,</p>
<p><span id="eq-unembed"><span class="math display">\[
T(t) = W_{U} x_{-1}
\tag{8}\]</span></span></p>
<p>which allow us to predict words. For our input sequence Phi-3 returned:</p>
<ul>
<li><code>London</code> with 76% probability</li>
<li><code>West</code> with 5% probability</li>
<li><code>B</code> with 4% probability</li>
<li>…</li>
</ul>
<p>We can however apply the unembedding to intermediate vectors in the residual stream, getting a crude understanding of what the model is thinking at that point. This allows us to see the most probably token next token for each token in the sequence and for each layer in the model.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/logit-lens.svg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>High level diagram of Logit Lens</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Tuned Lens
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>It turns out that Logit Lens is not very reliable. Each layer is free to represent features differently so long as the final layer corresponds to the unembedding. It works for some models but not all.</p>
<p>This problem is partially solved by the Tuned Lens <span class="citation" data-cites="Tuned-Lens">(<a href="#ref-Tuned-Lens" role="doc-biblioref">Belrose et al. 2023</a>)</span>. By accounting for a non-zero bias term and allowing a change of basis we get results that better match how Logit Lens orginally performed on GPT-2.</p>
</div>
</div>
</div>
<p>A great example of using a Logit Lens is the paper “Do Llamas Work in English? On the Latent Language of Multilingual Transformers” <span class="citation" data-cites="do-llamas-work-in-english">(<a href="#ref-do-llamas-work-in-english" role="doc-biblioref">Wendler et al. 2024</a>)</span>. Which gave models a translation task and applied Logit Lens to see if the model thought in the prompt language or English.</p>
<p>I have replicated this experiment using Phi-3 and a simple prompt where we expect the model to spot the pattern in French and return the correct flavour associated with the food,</p>
<div style="text-align: center; font-style: italic; font-size: 1.2em; margin: 20px 0;">
<p>“Citron: Acide<br>
Sucre: Sucré<br>
Piment: Épicé<br>
Miel:”</p>
</div>
<p>For this prompt we would expect a response <em>Doux</em> telling us that <em>Miel</em> (Honey) has a <em>Sweet/Mild</em> flavour. We run inference with the prompt and save the hidden states. Then we apply <span class="math inline">\(W_{U}\)</span> and a softmax<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> and find the token with the highest probability for each layer.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;The softmax function is <span class="math inline">\(\sigma(z_{i}) = \frac{e^{z_{i}}}{\sum e^{z_{j}}}\)</span></p></div></div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/logit_lens_viz.svg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Evolution of tokens seen with Logit Lens</figcaption>
</figure>
</div>
<p>In the plot above we can see a successful replication of <span class="citation" data-cites="do-llamas-work-in-english">(<a href="#ref-do-llamas-work-in-english" role="doc-biblioref">Wendler et al. 2024</a>)</span>. With rows representing the layers of the model and columns the tokens in the sequence. We see from layer 22 to 27 the model is thinking <code>sweet</code> in English and only in the last 4 layers does it pivot and prepare to output the correct French word <code>Doux</code>!</p>
</section>
<section id="linear-probes" class="level3">
<h3 class="anchored" data-anchor-id="linear-probes">Linear Probes</h3>
<p>Linear probes approach the interpretability problem in a different way. Imagine we have two classes of inputs, the linear probe is a classifier trained on the residual stream vectors that learns to distinguish these classes. Examples might include:</p>
<ul>
<li>Proper punctuation vs.&nbsp;ALL CAPS</li>
<li>Natural language vs.&nbsp;HTML formatting</li>
<li>Harmless Instructions vs.&nbsp;Harmful Instructions</li>
</ul>
<p>The first case is probably quite easy to learn: there is a simple pattern-matching problem for the model to spot between <code>"Hello, can I help you?"</code> and <code>"HELLO, CAN I HELP YOU?"</code>. Likewise, the second is hopefully quite easy but perhaps requires some more thinking: <code>"The main header of the page says Hello."</code> looks very different to <code>"&lt;h1 class='main-header'&gt; Hello &lt;/h1&gt;"</code>.</p>
<p>Examples of harmless and harmful text are more tricky to separate, requiring genuine semantic understanding. For example, <code>"There is a bug in your code, so you should **refactor the function.**"</code> is quite harmless whereas <code>"There is a bug in your code, so you should **disable the firewall.**"</code> could be catastrophic.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/linear-probes.svg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>High level diagram of Linear Probes</figcaption>
</figure>
</div>
<p>I ran three experiments across these example datasets. I used the simplest Linear Probe a logistic regression classifier. For a concept <span class="math inline">\(c\)</span> the probe <span class="math inline">\(p_{c}\)</span> is defined as,</p>
<p><span id="eq-logistic-regression"><span class="math display">\[
p_{c}= \sigma(W_{c} x + b_{c})
\tag{9}\]</span></span></p>
<p>which in PyTorch can be written as,</p>
<div id="39134675" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinearProbe(nn.Module):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Simple linear probe for binary classification."""</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, hidden_dim):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> nn.Linear(hidden_dim, <span class="dv">1</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sigmoid <span class="op">=</span> nn.Sigmoid()</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.linear(x)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        probs <span class="op">=</span> <span class="va">self</span>.sigmoid(logits)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> probs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>For each dataset I took just 50 pairs and trained probes on layer 16, the middle of Phi-3, for 200 epochs. We run inference through Phi-3 and save the hidden states <span class="math inline">\(x_{16}\)</span> on which we can train a linear probes. Results for all three are on the tabs below:</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" href="">ALL CAPS</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false" href="">HTML</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" role="tab" aria-controls="tabset-1-3" aria-selected="false" href="">Harmful</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/all_caps_results.svg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Loss and Accuracy for ALL CAPS Linear Probe</figcaption>
</figure>
</div>
<p>After some initial fluctuations by the 50th epoch the probe is certain when an input had proper punctuation or was in ALL CAPS.</p>
<p>I generated the ALL CAPS dataset by hand by taking permutations subjects, verbs and things <em>e.g.&nbsp;The cat jumps over the fence</em>.</p>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/html_formatting_results.svg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Loss and Accuracy for HTML Linear Probe</figcaption>
</figure>
</div>
<p>Learning to distinguish natural language and HTML is even easier, less than 25 epochs and the probe is certain!</p>
<p>For the natural language I used samples from an Alpaca dataset <span class="citation" data-cites="alpaca">(<a href="#ref-alpaca" role="doc-biblioref">Taori et al. 2023</a>)</span> and for the HTML an Alpaca HTML dataset <span class="citation" data-cites="alpaca_html">(<a href="#ref-alpaca_html" role="doc-biblioref">Bui 2023</a>)</span>.</p>
</div>
<div id="tabset-1-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-3-tab">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/harmful_instructions_results.svg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Loss and Accuracy for Harmful Instructions Linear Probe</figcaption>
</figure>
</div>
<p>Separating harmless and harmful instructions is, as expected, a little harder. By the 75th epoch however the probe is again certain whether a prompt is harmful or not.</p>
<p>For the harmless instructions I used samples from the an Alpaca dataset <span class="citation" data-cites="alpaca">(<a href="#ref-alpaca" role="doc-biblioref">Taori et al. 2023</a>)</span> and for the harmful instructions an advbench <span class="citation" data-cites="advbench">(<a href="#ref-advbench" role="doc-biblioref">Zou et al. 2023</a>)</span>.</p>
</div>
</div>
</div>
<p>Hopefully soon I will write a dedicated piece on Linear Probes, watch this space.</p>
</section>
</section>
<section id="references" class="level2">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Linear-Probes" class="csl-entry" role="listitem">
Alain, Guillaume, and Yoshua Bengio. 2018. <span>“Understanding Intermediate Layers Using Linear Classifier Probes.”</span> <a href="https://arxiv.org/abs/1610.01644">https://arxiv.org/abs/1610.01644</a>.
</div>
<div id="ref-shattered-gradients" class="csl-entry" role="listitem">
Balduzzi, David, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. 2018. <span>“The Shattered Gradients Problem: If Resnets Are the Answer, Then What Is the Question?”</span> <a href="https://arxiv.org/abs/1702.08591">https://arxiv.org/abs/1702.08591</a>.
</div>
<div id="ref-Tuned-Lens" class="csl-entry" role="listitem">
Belrose, Nora, Igor Ostrovsky, Lev McKinney, Zach Furman, Logan Smith, Danny Halawi, Stella Biderman, and Jacob Steinhardt. 2023. <span>“Eliciting Latent Predictions from Transformers with the Tuned Lens.”</span> <a href="https://doi.org/10.48550/arXiv.2303.08112">https://doi.org/10.48550/arXiv.2303.08112</a>.
</div>
<div id="ref-alpaca_html" class="csl-entry" role="listitem">
Bui, Anh-Tuan. 2023. <span>“Html_alpaca.”</span> <em>Hugging Face Hub</em>. <a href="https://huggingface.co/datasets/ttbui/html_alpaca" class="uri">https://huggingface.co/datasets/ttbui/html_alpaca</a>; Hugging Face.
</div>
<div id="ref-Mathematical-Framework-for-Transformer-Circuits" class="csl-entry" role="listitem">
Elhage, Nelson, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, et al. 2021. <span>“A Mathematical Framework for Transformer Circuits.”</span> <em>Transformer Circuits Thread</em>. <a href="https://transformer-circuits.pub/2021/framework/index.html">https://transformer-circuits.pub/2021/framework/index.html</a>.
</div>
<div id="ref-ResNet" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. <span>“Deep Residual Learning for Image Recognition.”</span> <em>CoRR</em> abs/1512.03385. <a href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a>.
</div>
<div id="ref-Fact-Finding" class="csl-entry" role="listitem">
Nanda, Neel, Senthooran Rajamanoharan, János Kramár, and Rohin Shah. 2024. <span>“Fact Finding: Attempting to Reverse Engineer Factual Recall in <span>LLMs</span>.”</span> August 2024. <a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall">https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall</a>.
</div>
<div id="ref-Logit-Lens" class="csl-entry" role="listitem">
nostalgebraist. 2020. <span>“Interpreting GPT: The Logit Lens.”</span> <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens">https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens</a>.
</div>
<div id="ref-3b1b-backpropagation" class="csl-entry" role="listitem">
Sanderson, Grant. 2017. <span>“Backpropagation Calculus | Deep Learning Chapter 4.”</span> <a href="https://www.3blue1brown.com/lessons/backpropagation-calculus" class="uri">https://www.3blue1brown.com/lessons/backpropagation-calculus</a>.
</div>
<div id="ref-alpaca" class="csl-entry" role="listitem">
Taori, Rohan, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. <span>“Alpaca: A Strong, Replicable Instruction-Following Model.”</span> <em>Hugging Face Hub</em>. <a href="https://huggingface.co/datasets/tatsu-lab/alpaca" class="uri">https://huggingface.co/datasets/tatsu-lab/alpaca</a>; Hugging Face.
</div>
<div id="ref-do-llamas-work-in-english" class="csl-entry" role="listitem">
Wendler, Chris, Veniamin Veselovsky, Giovanni Monea, and Robert West. 2024. <span>“Do Llamas Work in English? On the Latent Language of Multilingual Transformers.”</span> <a href="https://arxiv.org/abs/2402.10588">https://arxiv.org/abs/2402.10588</a>.
</div>
<div id="ref-Hyper-Connections" class="csl-entry" role="listitem">
Zhu, Defa, Hongzhi Huang, Zihao Huang, Yutao Zeng, Yunyao Mao, Banggu Wu, Qiyang Min, and Xun Zhou. 2025. <span>“Hyper-Connections.”</span> <a href="https://arxiv.org/abs/2409.19606">https://arxiv.org/abs/2409.19606</a>.
</div>
<div id="ref-advbench" class="csl-entry" role="listitem">
Zou, Andy, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. 2023. <span>“Universal and Transferable Adversarial Attacks on <span>A</span>ligned <span>L</span>anguage <span>M</span>odels.”</span> <a href="https://huggingface.co/datasets/S3IC/advbench" class="uri">https://huggingface.co/datasets/S3IC/advbench</a>; GitHub/Hugging Face.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/andrewstephenfletcher\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
©
<script>document.write(new Date().getFullYear())</script>
<p>Andrew Stephen Fletcher</p>
</div>   
    <div class="nav-footer-center">
<p>Built with <a href="https://quarto.org">Quarto</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>