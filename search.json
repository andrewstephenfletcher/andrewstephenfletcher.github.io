[
  {
    "objectID": "notebooks/hf.html",
    "href": "notebooks/hf.html",
    "title": "",
    "section": "",
    "text": "# Clear any existing invalid HF_TOKEN environment variable\nimport os\nif 'HF_TOKEN' in os.environ:\n    del os.environ['HF_TOKEN']\n    print(\"Cleared HF_TOKEN environment variable\")\n\n\nfrom huggingface_hub import login\nlogin()\n\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_name = \"meta-llama/Llama-3.2-1B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n\n\n\n\nimport torch.nn.functional as F\n\ninputs = tokenizer(\n    \"Fact: The capital of the country containing Manchester is\",\n    padding=True,\n    truncation=True,\n    return_tensors=\"pt\"\n)\n\nprint(f\"Tokenized {inputs['input_ids'].shape[0]} sequences with max length {inputs['input_ids'].shape[1]}.\")\n\nwith torch.no_grad():\n    outputs = model(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        output_hidden_states=True,\n    )\n\nlast_token_logits = outputs.logits[0, -1, :]\n\nprobs = F.softmax(last_token_logits, dim=-1)\n\ntop_k = 5\ntop_probs, top_indices = torch.topk(probs, k=top_k)\n\nprint(f\"Fact: the capital of the state containing Dallas is\",)\nprint(\"-\" * 30)\nfor i in range(top_k):\n    token = tokenizer.decode(top_indices[i])\n    probability = top_probs[i].item() * 100\n    print(f\"{i+1}. {token:10} | Confidence: {probability:.2f}%\")\n\nTokenized 1 sequences with max length 11.\nFact: the capital of the state containing Dallas is\n------------------------------\n1.  London    | Confidence: 12.89%\n2.  Birmingham | Confidence: 7.81%\n3.  not       | Confidence: 6.08%\n4.  Manchester | Confidence: 5.37%\n5.  called    | Confidence: 4.74%"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Documenting my learning of AI Safety.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Residual Stream\n\n\n\nmechanistic interpretability\n\nresidual stream\n\nlatent space monitoring\n\n\n\n\n\n\n\n\n\nFeb 10, 2026\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/residual-stream/index.html",
    "href": "blog/posts/residual-stream/index.html",
    "title": "The Residual Stream",
    "section": "",
    "text": "This post discusses the residual stream in transformer models and builds towards using it for two examples of mechanistic interpretability: Logit Lens (nostalgebraist 2020) and Linear Probes (Alain and Bengio 2018)."
  },
  {
    "objectID": "blog/posts/residual-stream/index.html#transformers",
    "href": "blog/posts/residual-stream/index.html#transformers",
    "title": "The Residual Stream",
    "section": "Transformers",
    "text": "Transformers\nMuch of the description of a transformer here comes straight from the excellent “A Mathematical Framework for Transformer Circuits” (Elhage et al. 2021).\nThe transformer is made of a few crucial components:\n\nToken embedding, \\(W_{E}\\)\nResidual Stream, \\(x_{i}\\)\nAttention layer, \\(Attn(x_{i}) = \\sum_{h \\in H} h(x_{i})\\)\nMLP Layer, \\(MLP(x_{i})\\)\nUnembedding, \\(W_{U}\\)\n\nI will give a high level overview of what some of these elements do by considering the input sequence,\n\n“The capital of the country containing Manchester1 is”\n1 Getting a sensible answer from small LLMs is hard, GPT-2 prompted with “The capital of the country containing Lyon is” returned “the” with 30% confidence and “France” with only 4% confidence.\nI follow the final token “is” as this is the token from which the model will predict the next token.\nThe first step is to use the Token embedding, \\(W_{E} \\in \\mathbb{R}^{d_{model} \\times d_{vocab}}\\), which acts as a look-up table from the token “is”, which we represent initially by the Token index \\(t \\in \\mathbb{R}^{d_{vocab}}\\), to the \\(d_{model}\\) vector that represents it,\n\\[\nx_{0} = W_{E} t\n\\tag{1}\\]\nThis is the source of the residual stream, each process in the model adds something to this initial vector \\(x_{0}\\)2. The vector \\(x_{0}\\) starts as an isolated embedding of the token “is” but finishes as a representation capable of predicting the next token in the sequence.\n2 There has been some progress towards not using the simple additive residual stream. One such example are Hyper-Connections (Zhu et al. 2025), which allow the strength of connections between layers to be learnt during training. With the residual stream a crucial tool for safety researchers I think it would be a major set back if connection variants such as this became the norm.\n\n\nHigh level diagram of the residual stream\n\n\nMost commonly it seems a layer3 of a transformer refers to the combination of an Attention layer and an MLP layer but they add to the residual stream sequentially,\n3 (Elhage et al. 2021) also refers to this as a “residual block”.\\[\nx_{n+1} = x_{n} + Attn(x_{n}) + MLP(x_{n} + Attn(x_{n}))\n\\tag{2}\\]\nAttention heads move information from the residual streams of other tokens in the sequence. For example in our example if we are going to answer the question,\n\n“The capital of the country containing Manchester is”\n\nour final representation \\(x_{-1}\\) is going to need to capture the multi-hop reasoning that the country containing Manchester is the United Kingdom and hence the capital is London we need to move information from these other tokens.\nIt must also know these geographical facts, research suggests that this is captured somewhere within the MLP layers (Nanda et al. 2024)."
  },
  {
    "objectID": "blog/posts/residual-stream/index.html#why-residuals",
    "href": "blog/posts/residual-stream/index.html#why-residuals",
    "title": "The Residual Stream",
    "section": "Why residuals?",
    "text": "Why residuals?\nThere are a few reasons that this additive approach is favoured but their inclusion is likely a legacy of the Residual Neural Network (ResNet) (He et al. 2015). This paper suggested a residual stream formulation,\n\\[\nx + f(x)\n\\tag{3}\\]\nwhich has since also become known as a skip connection. The layers of the network return a “residual mapping” \\(f(x)\\) which is added to the main input. From their paper the primary motivation was to combat the degradation problem, where deeper networks have higher training and test error. This should be slightly surprising as we might expect a model with 20 layers can be replicated within a model of 50 layers.\nIt turns out that for networks without a residual stream the correlation between gradients decays exponentially (Balduzzi et al. 2018), that is to say at a point \\(x\\) the gradient \\(\\nabla f(x)\\) might be almost random compared to a near by point \\(x + \\delta\\) with gradient \\(\\nabla f(x + \\delta)\\). If the gradients are almost random gradient descent is too, we set off down the slope only to immediately find it increase again. In contrast, with skip connections the correlation between gradients decays sub-linearly.\nA subtly different additional benefit to using a residual stream is that we also solve the vanishing gradient problem, where by in a deep network backpropagation will involve many applications of the chain rule such that if any of the gradients are small the total update to the weights will vanish.\nFollowing this 3Blue1Brown explainer (Sanderson 2017) we consider a very simple network with single neuron layers, connected in sequence. Let’s say every layer consists of,\n\\[\nf(x_{i}) = \\sigma(W x_{i}  + b)\n\\tag{4}\\]\nThe sigmoid function4, \\(\\sigma\\), is defined as,\n4 ReLU and GELU activation functions are designed to also combat the vanishing gradient problem.\\[\n\\sigma(z) = \\frac{e^z}{1+e^z}\n\\tag{5}\\]\nIts derivative, \\(\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\\), is bounded within the interval \\((0, 0.25]\\). Hence, when we calculate the gradient of the loss, \\(\\nabla \\mathcal{L}\\), in backpropagation this term will reduce the resulting value by at least a factor of 4 each layer. As we update the weights according to \\(-\\eta \\nabla \\mathcal{L}\\) when the gradient vanishes we stop learning.\nWith a residual stream there will always be a non-zero gradient,\n\\[\nx_{i+1} = x_{i} + f(x_{i})\n\\tag{6}\\]\n\\[\n\\frac{\\partial(x_{i+1})}{\\partial x_{i}} = 1 + f'(x_{i})\n\\tag{7}\\]\nThe constant term \\(1\\) means even with repeated applications of the chain rule it is always possible to trace through the network to get a non-zero update to earlier layers."
  },
  {
    "objectID": "blog/posts/residual-stream/index.html#the-residual-stream-in-mechanistic-interpretability",
    "href": "blog/posts/residual-stream/index.html#the-residual-stream-in-mechanistic-interpretability",
    "title": "The Residual Stream",
    "section": "The Residual Stream in Mechanistic Interpretability",
    "text": "The Residual Stream in Mechanistic Interpretability\nWith this consistent \\(d_{model}\\) dimension vector flowing through the model we have an obvious place to inspect the model’s thoughts. As each layer refines the representation of the “is” token in preparation for predicting the next we can explore two ways to interpret this process:\n\nLogit Lens (nostalgebraist 2020)\nLinear Probes (Alain and Bengio 2018)\n\nIn the rest of this post I have some experiments that use these techniques to illustrate their value.\n\nLogit Lens\nThe idea beyhind Logit Lens is simple, after the final layer, \\(x_{-1}\\), of the transformer we apply an unembedding to get logits,\n\\[\nT(t) = W_{U} x_{-1}\n\\tag{8}\\]\nwhich allow us to predict words. For our input sequence GPT-2 returned:\n\nLondon with 13% confidence\nBirmingham with 8% confidence\nnot with 6% confidence\n…\n\nWe can however apply the unembedding to intermediate vectors in the residual stream, getting a crude understanding of what the model is thinking of at that point. This allows us, in a very simple way, to see the thoughts of the model progress through the layers.\n\n\n\nHigh level diagram of Logit Lens\n\n\n\n\n\n\n\n\nNoteTuned Lens\n\n\n\n\n\nIt turns out that logit lens is not very reliable, when we apply the unembedding we find something uninformative such as the input token repeated.\nThis problem is partially solved by the Tuned Lens (Belrose et al. 2023).\nBy accounting for a non-zero bias term and allowing a change of basis we get results that better match how Logit Lens performed on GPT-2.\n\n\n\nA great example of using a Logit Lens is the paper “Do Llamas Work in English? On the Latent Language of Multilingual Transformers” (Wendler et al. 2024). Which gave models a translation task and applied Logit Lens to see if the model thought in prompt language or English.\nI have replicated this experiment using Phi-3 and a simple prompt where we expect the model to spot the pattern in French and return the correct flavour associated with the food,\n\n“Citron: Acide\nSucre: Sucré\nPiment: Épicé\nMiel:”\n\nWhere we would expect a response Doux telling us that Miel/Honey has a Sweet/Mild flavour.\n\n\n\nEvolution of tokens seen with Logit Lens\n\n\nIn the images above we can see a successful replication of (Wendler et al. 2024), from layer 22 to 27 the model is thinking sweet in English only in the last 4 layers does it pivot and prepare to output the correct French word Doux!\n\n\nLinear Probes\nLinear probes approach the interpretability problem in a different way. Imagine we have two classes of inputs, the linear probe is a classifier trained on the residual stream vectors that learns to distinguish the classes. Examples might include:\n\nProper punctuation vs. ALL CAPS\nNatural language vs. HTML\nHarmless Instructions vs. Harmful Instructions\n\nThe first case we might imagine is quite simple to learn: there is a simple pattern-matching problem for the model to spot between \"Hello, can I help you?\" and \"HELLO, CAN I HELP YOU?\". Likewise, the second is hopefully quite easy to discern, but perhaps requires some more thinking: \"The main header of the page says Hello.\" looks very different to \"&lt;h1 class='main-header'&gt; Hello &lt;/h1&gt;\", but it requires more knowledge to consistently separate the two.\nFinally, examples of harmless and harmful text might be really tricky to separate, requiring genuine semantic understanding. For example, \"There is a bug in your code, so you should **refactor the function.**\" is quite harmless whereas \"There is a bug in your code, so you should **disable the firewall.**\" could be catastrophic.\n\n\n\nHigh level diagram of Linear Probes\n\n\nI ran three experiments across these example datasets. I used the simplest Linear Probe a logistic regression such that for a concept \\(c\\) the probe \\(p_{c}\\) is defined as,\n\\[\np_{c}= \\sigma(\\mathbf{w_{c}}^\\top \\mathbf{x} + b_{c})\n\\tag{9}\\]\nwhich in PyTorch can be written as,\n\nimport torch.nn as nn\n\nclass LinearProbe(nn.Module):\n    \"\"\"Simple linear probe for binary classification.\"\"\"\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.linear = nn.Linear(hidden_dim, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        logits = self.linear(x)\n        probs = self.sigmoid(logits)\n        return probs\n\nFor each dataset I took just 50 pairs and trained probes on layer 16, the middle of Phi-3, for 200 epochs. We run inference through Phi-3 and save the hidden states \\(\\mathbf{x}_{16}\\) on which we can train a linear probes. Results for all three are on the tabs below:\n\nALL CAPSHTMLHarmful\n\n\n\n\n\nLoss and Accuracy for ALL CAPS Linear Probe\n\n\nAfter some initial fluctuations by the 50th epoch the probe is certain when an input was normal text or in ALL CAPS.\nI generated the ALL CAPS dataset by hand by taking permutations subjects, verbs and things e.g. The cat jumps over the fence.\n\n\n\n\n\nLoss and Accuracy for HTML Linear Probe\n\n\nLearning to distinguish normal text and HTML is even easier, less than 25 epochs and the probe is certain!\n\n\n\n\n\nLoss and Accuracy for Harmful Instructions Linear Probe\n\n\nSeparating harmless and harmful instructions is, as expected, a little harder. By the 75th epoch however the probe is again certain whether a prompt is harmful or not."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrew Stephen Fletcher",
    "section": "",
    "text": "I am a Data Scientist with an interest in AI Safety research. I am keen to learn more about many areas but have started diving into Latent Space Monitoring. This website has a blog, mainly focused on explaining AI safety techniques as I learn them."
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Andrew Stephen Fletcher",
    "section": "",
    "text": "I am a Data Scientist with an interest in AI Safety research. I am keen to learn more about many areas but have started diving into Latent Space Monitoring. This website has a blog, mainly focused on explaining AI safety techniques as I learn them."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Andrew Stephen Fletcher",
    "section": "Education",
    "text": "Education\nUniversity of Exeter | Data Science MSc\nDeep learning and PyTorch | 2023 - present\nUniversity of Oxford | Physics MPhys\nFocus on Astrophysics | 2019 - 2023"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Andrew Stephen Fletcher",
    "section": "Experience",
    "text": "Experience\nBBC Studios | Data Science\nML research predicting TV viewing | 2023 - present\nBlueDot | Technical AI Safety Course\nCourse introducing ideas in technical AI safety | 2025"
  },
  {
    "objectID": "docs/blog/posts/residual-stream/residual_stream_animation.html",
    "href": "docs/blog/posts/residual-stream/residual_stream_animation.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\n\n\nd_model = 768\nn_layers = 12\n\nresidual_stream = np.random.randn(n_layers, d_model)\n\nresidual_stream.shape\n\n(12, 768)\n\n\nLet’s use Einops to rearrange this into a nice array to visualize.\n\nfrom einops import rearrange, reduce, repeat\n\nresidual_stream = rearrange(residual_stream, \"layer (row col) -&gt; layer row col\", row=16)\n\nresidual_stream.shape\n\n(12, 16, 48)\n\n\nNow we do the simplest plot of such a tensor.\n\nimport matplotlib.pyplot as plt\n\nplt.imshow(residual_stream[0], cmap='viridis')\nplt.title(\"Residual Stream at Layer 0\")\nplt.show()\n\n\n\n\n\n\n\n\nWe want to make this much prettier for the blog.\n\nfig, ax = plt.subplots(1,1, figsize=(15,5))\nax.axis(False)\nfig.set_facecolor('black')\nax.set_facecolor('black')\n\ncmesh = ax.pcolormesh(residual_stream[0], edgecolors='k', cmap='viridis', vmin=-3, vmax=3)\n\n\n\n\nAnimation of the Residual Stream Across Layers\n\n\n\n\nNow we can think about animating this.\n\nfrom matplotlib.animation import FuncAnimation\n\ndef anim_function(frame_num):\n    cmesh.set_array(residual_stream[frame_num,:,:])\n    return cmesh,\n\nanim = FuncAnimation(fig, anim_function, frames=np.arange(residual_stream.shape[0]), interval=30)\n\nfrom IPython.display import HTML\nHTML(anim.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nNow lets look to do something with actual model activations!\n\nfrom huggingface_hub import login\nlogin()\n\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n\n\n\n\n\n\nGPT2LMHeadModel LOAD REPORT from: gpt2\nKey                  | Status     |  | \n---------------------+------------+--+-\nh.{0...11}.attn.bias | UNEXPECTED |  | \n\nNotes:\n- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n\n\n\ninputs = tokenizer(\n    \"Paris is the capital of\",\n    padding=True,\n    truncation=True,\n    return_tensors=\"pt\"\n)\n\nprint(f\"Tokenized {inputs['input_ids'].shape[0]} sequences with max length {inputs['input_ids'].shape[1]}.\")\n\nTokenized 1 sequences with max length 5.\n\n\n\nwith torch.no_grad():\n    outputs = model(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        output_hidden_states=True,\n    )\n\n\ntype(outputs.hidden_states)\n\ntuple\n\n\n\noutputs.hidden_states[12][0, -1, :]\n\ntensor([ 3.9707e-01,  6.5592e-01, -1.0789e-01, -5.1339e-01,  2.2478e-01,\n        -3.5340e-01,  2.9402e+00,  5.0266e-01, -1.1648e-01, -3.3925e-01,\n        -1.4641e-01,  1.3185e-01,  5.6856e-01, -2.4193e-01,  2.3383e-01,\n        -6.1204e-01, -1.6270e-01,  2.6729e-01,  1.0813e-01, -3.3550e+00,\n         3.8122e-01,  3.1480e-01,  3.6961e-01, -1.1843e-01, -4.1384e-02,\n         2.7716e-01, -6.5405e-01, -6.8845e-01,  1.7058e-01,  2.3255e-01,\n         2.1136e-02, -2.7776e-01,  1.7016e-01,  3.5261e-01,  6.2866e-01,\n        -2.2970e-01,  5.0996e+01, -3.4832e-01, -4.9283e-01,  1.6009e-01,\n        -2.5912e-01, -4.3954e-02, -6.6652e-02,  1.7371e-02, -4.9989e-02,\n         7.7331e-02, -2.0900e-01, -5.2491e-01, -9.1295e-01, -7.5307e-02,\n        -5.9557e-02, -3.5249e-01,  9.1986e-02, -4.4967e-02,  1.7474e-01,\n         1.0622e+00, -3.1931e-02, -2.3144e-01,  1.4521e-01, -4.0620e-01,\n        -5.8457e-01,  1.4042e-02,  1.0694e-01, -1.2151e-01, -1.1069e+00,\n         4.4808e-01, -1.6879e-01,  4.8419e-02, -1.2701e+00,  1.9134e-01,\n        -7.3997e-01, -6.9388e-01,  9.3711e-03, -3.0020e-01, -2.9383e-02,\n         1.1450e-01,  2.2709e-01, -6.2942e-01,  3.4850e-01,  4.6560e-01,\n         2.9459e-01, -2.3642e-01,  4.6218e-01, -7.3631e-02,  4.3862e-01,\n         1.2286e-01,  9.8999e-01, -1.0245e+00,  2.4983e-01, -1.2601e-01,\n         4.3485e-01,  6.3421e-01,  3.8452e-01,  9.8958e-03, -6.1150e-02,\n        -2.8366e-01, -3.5584e-01, -3.4806e-01,  4.6458e-02,  1.0344e-01,\n        -1.7640e-01,  2.6008e-01,  3.0842e-01, -5.9029e-01,  6.2168e-02,\n         8.0234e-02, -2.1006e-01, -1.3377e+00,  4.7618e-01, -2.3790e-01,\n         5.6921e-01, -1.8896e-01,  8.6591e-02,  1.3231e-02,  1.2710e-01,\n        -5.3515e-01,  3.2699e-01, -2.2347e-01,  2.8715e-01, -1.1292e+00,\n         7.7704e-02,  5.1702e-01,  2.5250e-02,  8.0246e-01, -2.2748e-01,\n         4.8657e-01,  4.7917e-01,  5.0834e-01, -1.8833e-01,  5.0228e-01,\n        -3.7779e-01, -3.9931e-01,  2.4244e-01,  2.1779e-01, -1.0248e-01,\n         4.3215e-01,  1.9822e-01, -4.4591e-01,  5.6154e-01,  3.0105e-01,\n        -4.6189e-01, -8.0389e-01,  9.8432e-01, -3.1944e-01,  9.4833e-02,\n         1.1400e-01, -1.5258e-01,  8.6682e-02, -5.9506e-03,  1.0061e+00,\n         3.5015e-01, -5.4344e-01, -1.4929e-02,  2.9061e-01,  7.7716e-01,\n         5.7593e-02,  4.1433e-01,  2.0678e-01,  1.1295e-01, -3.3525e-01,\n        -6.4733e-01, -2.0853e-01, -6.8152e-01, -2.3764e-01, -7.2244e-03,\n         5.3895e-02, -1.7540e-02, -3.5193e+00, -2.7689e-01, -1.4060e-01,\n         2.2860e-01, -2.3910e-01,  1.4758e-01,  4.6928e-01, -4.1897e-01,\n        -8.1610e-01,  6.8833e-01, -5.3819e-01, -5.4749e-02, -1.4693e-01,\n         1.5520e-01,  3.4751e-01, -1.0811e-02, -1.1496e-01, -3.7802e-01,\n        -5.4832e-02, -1.7603e-01, -1.5183e-01, -3.5338e-03, -1.5314e-02,\n        -1.0327e-01, -4.9390e-01, -9.5211e-03,  2.2363e-01, -8.6061e-02,\n        -1.6430e+00,  4.8663e-01,  2.2926e-01, -1.5008e-01,  4.6846e-03,\n         1.8621e-01,  5.8974e-01, -2.3210e-01, -3.8068e-02,  5.2106e-02,\n        -2.1625e-01,  1.3113e-01, -1.6528e-01, -3.0899e-01, -8.6106e-02,\n         1.1730e-01, -2.2622e-01, -3.6722e-01, -1.5751e-01, -3.6578e-02,\n        -3.0102e-01, -4.9017e-02, -4.6044e-01,  4.7928e-01,  3.5405e-02,\n         9.5526e-02, -1.9290e-01,  3.5486e-01,  3.5565e-01, -4.3436e-01,\n        -3.7214e-01,  3.7490e-01, -1.4289e-01,  1.4892e-01,  4.4548e-01,\n        -6.1742e-02,  3.3417e-01,  1.5622e+00, -1.6550e-01, -9.5622e-02,\n        -3.1609e-01,  3.5710e-01,  1.5547e-01,  3.7913e-01, -4.5316e-01,\n        -3.0411e-01,  6.3645e-01,  4.6493e-01, -2.7021e-01,  2.8007e-01,\n         3.5020e-01,  4.3037e-01, -2.0467e-01, -1.7361e-01,  5.0707e-02,\n         5.2277e-01, -1.6470e-01,  2.9059e-01,  9.7539e-02, -8.1670e-02,\n         3.0347e-01,  3.0409e-01, -1.9271e-01, -3.8301e-01,  5.5685e-01,\n        -2.9989e-01,  7.5335e-01, -1.9442e-01, -8.1730e-01, -5.7642e-02,\n         4.5157e-01, -1.0122e+00,  3.4106e-02,  3.7177e-01,  4.8461e-01,\n         1.0395e+00,  3.5976e-01,  1.9491e-01, -1.8765e-01,  9.0858e-02,\n        -1.2970e-01, -6.9582e-02, -3.3006e-01,  1.2637e-01, -1.4938e-01,\n         1.7854e-01, -3.3438e-02,  2.9249e-01, -6.0153e-02, -3.9808e-01,\n        -4.7156e-02,  1.6853e-01, -2.4003e-02, -9.1483e-02,  3.4247e-01,\n         2.2795e-01,  3.3932e-01, -3.3874e-01, -5.3320e-02, -1.2861e-02,\n        -1.2411e-02,  3.7387e-02,  5.8254e-02, -2.5397e-01,  5.2348e-01,\n         8.6658e-01, -3.5760e-01,  1.3084e-01, -1.1010e-01,  5.1624e-01,\n         6.0349e-01, -4.8093e-01,  1.6971e-01,  1.2630e+00, -2.6303e-01,\n         1.8936e-01,  2.1825e-01,  2.9491e-02, -3.4005e-01, -4.3871e+01,\n         4.7595e-02, -1.4228e-01, -1.0846e-01, -1.5160e-01,  1.0329e-01,\n         3.7053e-01, -1.2320e-02,  3.1999e-01,  5.6906e-02,  3.0835e-01,\n         2.0784e-01, -3.3139e-01, -4.5700e-01, -1.5465e-01,  1.6962e-01,\n        -3.5235e-01, -7.5174e-03,  4.5752e-01,  4.0825e-01, -1.1673e-01,\n        -1.0488e-02,  2.5212e-02, -1.6757e-01,  2.3002e-01, -1.8673e-02,\n        -1.7697e-01, -2.1341e-01, -6.6391e-01, -1.8274e-01,  7.0124e-01,\n         2.5572e-01,  1.5160e-01, -2.0538e-01,  1.8113e-01, -4.6497e-02,\n        -2.5627e-01, -4.6529e-01,  1.9979e-01, -1.2916e-01, -3.4473e-01,\n        -4.8907e-01, -3.6988e-01,  1.0921e-01, -1.2943e-01, -1.2346e-01,\n         8.8096e-02,  1.2546e+00,  6.3094e+00, -2.5674e-01, -4.3520e-01,\n        -2.3859e+00,  4.8746e-01, -3.9125e-02, -4.1557e-02, -2.7922e-01,\n         8.7475e-02,  2.1376e-01, -6.5235e-02, -1.2277e+00, -2.0121e+01,\n         4.7794e-01,  3.3857e-01,  1.3460e-01,  4.3976e-01,  1.7539e-01,\n         2.3180e-01, -3.4298e-01,  2.6690e-01, -3.0966e-01, -8.0459e-01,\n         1.0486e-01,  2.3068e-01, -2.7034e-01, -7.2113e-02,  1.2232e+00,\n        -3.3541e-02,  1.2416e-01, -5.4055e-02,  3.2458e-01, -2.6819e-01,\n        -2.7736e-01, -1.0285e-01, -3.2138e-01,  4.5720e-01,  1.7563e-01,\n        -5.7128e-02,  1.5178e-01, -4.0476e-01,  5.8006e-02,  3.9195e-02,\n        -2.5981e-01,  2.0811e-01, -2.8853e-01,  6.6633e+00, -5.7252e-01,\n        -5.5131e-02,  7.3943e-02,  3.7546e-01,  5.8285e-02, -6.3121e-01,\n        -5.0900e-01, -5.1948e-02, -1.1226e-01, -1.6612e-01, -1.6518e-01,\n         4.8815e-01,  1.6613e-01, -4.5851e-01, -5.6316e-01, -3.2561e-02,\n        -1.4305e-01, -2.9152e-01, -6.2530e-01, -1.1286e-01,  6.0920e-03,\n         7.0946e+01,  4.8999e-01,  1.3975e-01, -1.2525e-01, -1.5065e-01,\n         1.6868e-01, -2.7682e-02, -4.1910e-01,  2.6702e-01, -1.3076e+00,\n         4.4607e-01,  1.2542e-01, -2.8018e+00,  1.4158e-01, -5.2851e-01,\n        -3.3754e-01,  1.9284e-02,  1.0982e+00,  6.1203e-02,  2.5273e-01,\n        -6.5044e-01, -1.6463e-01, -3.6856e-01,  7.3381e-02,  2.8394e-01,\n         7.1713e-01, -4.5587e-01, -3.6499e-01, -3.1927e-01,  2.5299e-01,\n         3.1554e-01, -3.9655e-01, -1.6431e-01, -3.2882e-01,  4.8321e-01,\n         2.6438e-02,  3.2431e-01, -7.3670e-01,  3.7471e-01,  2.6125e-01,\n         1.1955e-01, -2.1661e-01,  4.1328e-01,  1.6487e-01,  4.8600e-02,\n        -1.4378e-02, -7.9637e-02, -4.6863e-01, -4.1781e-01,  1.7027e+00,\n        -8.9309e-01,  1.2432e+00,  2.1844e-01,  5.7696e-02,  1.4689e-01,\n         1.5076e-01,  5.9482e-02, -6.2378e-01, -2.9110e-01, -1.6801e-01,\n        -3.6049e-01, -3.8028e-02, -2.2511e-01, -5.7139e-01, -4.8377e-01,\n        -4.3101e-02,  1.8708e+02, -2.1507e-01,  1.9170e-01, -3.0552e-02,\n         2.2827e-01,  1.0583e-01, -7.0139e-02, -2.7798e+00,  1.4771e-01,\n         8.5176e-02, -2.1687e-01,  3.3596e-01,  5.2070e-01,  1.1699e-01,\n         9.5625e-02,  7.2405e-02, -7.4354e-01, -4.1796e-01, -3.1036e-01,\n         1.9051e-01,  1.3422e-01,  1.3123e-02,  6.9384e-02,  4.9266e-01,\n        -3.0280e-01,  4.1873e-01,  3.8501e-01,  5.0008e-02, -4.1961e-01,\n        -9.2081e-01, -8.8505e-01, -4.4598e-01,  3.2327e-01,  3.3958e-01,\n         2.7451e-01,  1.5717e-02, -1.3682e-01, -1.1711e-01,  8.5470e-01,\n         2.0400e-01, -3.2440e-03, -6.6013e-01, -9.5547e-01,  1.1471e-01,\n        -2.7788e-01, -2.0668e-01,  2.4634e-01,  6.9303e-01, -4.9175e-01,\n         5.9871e-01,  2.5664e-01, -3.5525e-02, -6.6084e-01,  3.5962e-03,\n         7.7517e-01,  3.3645e-01,  4.1101e-01,  6.3882e-01, -1.1079e+00,\n         3.5268e-01, -1.6977e-01, -3.6987e-02,  2.5442e-01,  3.5130e-01,\n        -3.1887e-02, -4.1854e-01,  1.6195e-01, -1.2116e-01,  3.4556e-01,\n        -3.1018e-01, -1.4006e-01, -2.2971e-02, -2.4656e-01, -1.3791e-02,\n        -7.8256e-02,  1.3936e-01,  2.5287e-01,  1.2679e-01,  1.0444e-02,\n        -7.2022e-01,  6.0788e-02,  1.3312e-01, -5.9214e-01, -2.5595e-02,\n        -1.9300e-01,  2.0226e-01,  8.7149e-02, -2.8428e-01, -7.0516e-02,\n         2.0947e-01, -2.4657e-01, -2.4485e-01, -9.9435e-02,  3.1029e-01,\n        -3.1992e-02, -6.6761e-02, -6.7534e-01, -8.6753e-01, -6.0732e-02,\n         1.5726e-01,  7.2841e-02, -1.3594e-01,  2.1770e-01, -1.9269e-01,\n         7.0538e-02,  2.4697e-01, -2.1211e-01,  3.9387e-01, -1.1869e-01,\n         3.3987e-01, -2.5016e-01,  3.6958e-02,  1.6831e+00,  1.3615e-02,\n        -1.7321e-01,  5.6813e-01,  6.5152e-01, -1.0398e-01,  3.8231e-01,\n         3.3899e-01,  8.4053e-01,  1.7321e-01,  2.9557e-01,  4.3555e-01,\n        -3.8328e-01,  7.0625e-01,  1.4007e-01,  5.9916e-01,  3.2180e-01,\n         2.8273e-01,  5.9825e-02,  9.4940e-01, -4.5199e-02,  2.8532e-01,\n        -7.0141e-02,  5.2700e-02, -1.2425e-01, -4.9037e-02, -3.8386e-01,\n        -6.8675e-01,  5.6606e-02,  1.1794e-01, -4.0015e-01,  1.5555e-01,\n         4.9732e-01, -2.5473e-02,  6.1942e-01, -4.3914e-01,  3.6841e-01,\n        -1.0106e+00,  5.7044e-01,  6.3400e-01,  1.8109e-01, -1.2689e+00,\n         3.0568e-01, -2.4669e-01, -4.7047e-01, -6.9111e-01, -1.8980e-01,\n         9.3604e-01, -2.0410e-01, -1.1872e-01, -2.7357e-02,  2.3875e-01,\n         3.1092e-01,  1.1390e-02, -2.4784e-01, -7.8412e-03, -6.9789e-01,\n         1.5111e-01, -2.7125e-01, -2.5387e-01, -1.8569e-01,  5.4281e-01,\n        -1.2238e-01,  5.8334e-01, -1.0270e-01, -6.6581e-01,  5.0467e-02,\n         2.2914e-01, -6.2707e-01,  3.1402e-01, -2.5495e-01,  2.1849e-01,\n        -8.2420e-01,  1.0366e-01, -6.9238e-02, -1.6138e-01,  2.4286e-02,\n         8.9909e-02, -2.2955e-02,  1.1647e-01,  3.7806e-01,  3.2999e-01,\n         9.7268e-01,  2.0536e-01,  3.2675e-01,  1.2438e-01,  9.5249e-02,\n         2.0198e-01, -1.1161e-01, -1.8637e-01, -5.8571e-02, -4.2029e-01,\n        -7.9599e-02,  1.5202e-01,  2.0047e-01, -1.0811e+00, -6.1192e-01,\n        -1.4036e-01,  5.7603e-01, -1.3407e-01, -1.2577e-01, -3.3293e-01,\n        -1.4304e-01, -5.7476e-01, -3.0210e-01, -1.0654e-01,  1.8689e-01,\n        -1.1735e-01,  2.6910e-02,  3.3609e-01, -3.8913e-01,  1.3305e-02,\n        -9.7757e-01, -5.0708e-01, -3.9805e-01, -2.5544e-01,  5.2961e-01,\n         7.5593e-01,  2.7601e-01, -1.0440e+00,  4.2858e-01, -3.3402e-01,\n         4.9627e-01, -5.5644e-02,  2.6034e-01, -3.1860e-01,  1.9822e-01,\n        -6.6199e-01,  5.5858e-01, -1.4615e-01, -1.3237e-01, -4.9860e-02,\n         6.2007e-02, -1.5592e-02,  1.9362e-01, -3.9611e-01, -5.7654e-01,\n        -1.5250e+00, -1.2473e-01,  1.1547e-01,  1.7411e-02, -4.1657e-01,\n        -3.4960e-02,  3.0048e-01, -3.1938e-01,  4.4102e-01, -6.5498e-01,\n         3.7197e-01, -1.3987e-01,  1.6034e-01, -6.5139e-01,  3.1342e-01,\n        -2.8109e-01,  1.6483e-01,  1.8485e-01, -7.5067e-01,  1.2059e-01,\n         2.5470e-01, -8.5622e-02,  4.0347e-02])\n\n\nNow we animate through the residual stream.\n\nresidual_stream = np.zeros((13, d_model))\n\nfor layer in range(13):\n    residual_stream_layer = outputs.hidden_states[layer][0, -1, :].numpy()\n    residual_stream[layer] = residual_stream_layer\n    print(f\"Layer {layer}: Residual stream shape {residual_stream_layer.shape}\")\n\nLayer 0: Residual stream shape (768,)\nLayer 1: Residual stream shape (768,)\nLayer 2: Residual stream shape (768,)\nLayer 3: Residual stream shape (768,)\nLayer 4: Residual stream shape (768,)\nLayer 5: Residual stream shape (768,)\nLayer 6: Residual stream shape (768,)\nLayer 7: Residual stream shape (768,)\nLayer 8: Residual stream shape (768,)\nLayer 9: Residual stream shape (768,)\nLayer 10: Residual stream shape (768,)\nLayer 11: Residual stream shape (768,)\nLayer 12: Residual stream shape (768,)\n\n\n\nresidual_stream = rearrange(residual_stream, \"layer (row col) -&gt; layer row col\", row=16)\n\nresidual_stream.shape\n\n(13, 16, 48)\n\n\n\nfrom ipywidgets import interact, interactive, IntSlider, VBox, HBox, Output\nimport matplotlib.pyplot as plt\n\noutput = Output()\n\ndef plot_layer(layer=0):\n    with output:\n        output.clear_output(wait=True)\n        fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n        ax.axis(False)\n        fig.set_facecolor('white')\n        ax.set_facecolor('white')\n\n        cmesh = ax.pcolormesh(residual_stream[layer], edgecolors='white', cmap='viridis', vmin=-3, vmax=3)\n        ax.set_title(f'Layer {layer}', color='black', fontsize=16)\n        plt.show()\n\nslider = IntSlider(min=0, max=residual_stream.shape[0]-1, step=1, value=0, description='Layer:')\nslider.observe(lambda change: plot_layer(change['new']), names='value')\n\n# Initial plot\nplot_layer(0)\n\n# Display with slider below and centered\nfrom ipywidgets import Layout\ndisplay(VBox([output, HBox([slider], layout=Layout(justify_content='center'))],\n             layout=Layout(background_color='white')))"
  },
  {
    "objectID": "projects/safety-viz.html",
    "href": "projects/safety-viz.html",
    "title": "Reward Misspecification Plot",
    "section": "",
    "text": "Here is a quick Python test to verify my interactive elements:\nimport matplotlib.pyplot as plt import numpy as np\nx = np.linspace(0, 10, 100) y = np.exp(x / 3) # Exponential ‘risk’ plot\nplt.plot(x, y, color=‘red’) plt.title(“Agent Risk Profile”) plt.show()"
  },
  {
    "objectID": "posts/paper-title/index.html#abstract",
    "href": "posts/paper-title/index.html#abstract",
    "title": "Can quantum-mechanical description of physical reality be considered complete?",
    "section": "Abstract",
    "text": "Abstract\nIn a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete."
  },
  {
    "objectID": "posts/paper-title/index.html#links",
    "href": "posts/paper-title/index.html#links",
    "title": "Can quantum-mechanical description of physical reality be considered complete?",
    "section": "Links",
    "text": "Links\nPublished paper"
  },
  {
    "objectID": "posts/news-title/index.html",
    "href": "posts/news-title/index.html",
    "title": "News item",
    "section": "",
    "text": "About the news"
  },
  {
    "objectID": "posts/news-title/index.html#summary",
    "href": "posts/news-title/index.html#summary",
    "title": "News item",
    "section": "",
    "text": "About the news"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "AI Safety Projects",
    "section": "",
    "text": "Interactive experiments and code visualizations.\n\n\n\n\n\n\n\n\n\n\n\n\nReward Misspecification Plot\n\n\nA visualization of agent behavior under faulty reward functions.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n      Default\n      \n      \n      \n        Title\n      \n      \n        Team\n      \n      \n        Funder\n      \n      \n        Description\n      \n      \n        Start\n      \n      \n        End\n      \n      \n        Funding\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n \n\n\n\nTitle\n\n\n\nTeam\n\n\n\nFunder\n\n\n\nDescription\n\n\n\nFunding\n\n\n\nStart\n\n\n\nEnd\n\n\n\n\n\n\n\n\n\n\n\nExploring \n\n\nAcademic Website\n\n\nFunder\n\n\nThe goal of this project is to investigate .\n\n\n$0\n\n\n2024\n\n\n2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/blog/posts/residual-stream/residual_stream_animation.out.html",
    "href": "docs/blog/posts/residual-stream/residual_stream_animation.out.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\n\n\nd_model = 768\nn_layers = 12\n\nresidual_stream = np.random.randn(n_layers, d_model)\n\nresidual_stream.shape\n\n(12, 768)\n\n\nLet’s use Einops to rearrange this into a nice array to visualize.\n\nfrom einops import rearrange, reduce, repeat\n\nresidual_stream = rearrange(residual_stream, \"layer (row col) -&gt; layer row col\", row=16)\n\nresidual_stream.shape\n\n(12, 16, 48)\n\n\nNow we do the simplest plot of such a tensor.\n\nimport matplotlib.pyplot as plt\n\nplt.imshow(residual_stream[0], cmap='viridis')\nplt.title(\"Residual Stream at Layer 0\")\nplt.show()\n\n\n\n\n\n\n\n\nWe want to make this much prettier for the blog.\n\nfig, ax = plt.subplots(1,1, figsize=(15,5))\nax.axis(False)\nfig.set_facecolor('black')\nax.set_facecolor('black')\n\ncmesh = ax.pcolormesh(residual_stream[0], edgecolors='k', cmap='viridis', vmin=-3, vmax=3)\n\n\n\n\nNow we can think about animating this.\n\nfrom matplotlib.animation import FuncAnimation\n\ndef anim_function(frame_num):\n    cmesh.set_array(residual_stream[frame_num,:,:])\n    return cmesh,\n\nanim = FuncAnimation(fig, anim_function, frames=np.arange(residual_stream.shape[0]), interval=30)\n\nfrom IPython.display import HTML\nHTML(anim.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nNow lets look to do something with actual model activations!\n\nfrom huggingface_hub import login\nlogin()\n\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n\n\n\n\n\n\nGPT2LMHeadModel LOAD REPORT from: gpt2\nKey                  | Status     |  | \n---------------------+------------+--+-\nh.{0...11}.attn.bias | UNEXPECTED |  | \n\nNotes:\n- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n\n\n\ninputs = tokenizer(\n    \"Paris is the capital of\",\n    padding=True,\n    truncation=True,\n    return_tensors=\"pt\"\n)\n\nprint(f\"Tokenized {inputs['input_ids'].shape[0]} sequences with max length {inputs['input_ids'].shape[1]}.\")\n\nTokenized 1 sequences with max length 5.\n\n\n\nwith torch.no_grad():\n    outputs = model(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        output_hidden_states=True,\n    )\n\n\ntype(outputs.hidden_states)\n\ntuple\n\n\n\noutputs.hidden_states[12][0, -1, :]\n\ntensor([ 3.9707e-01,  6.5592e-01, -1.0789e-01, -5.1339e-01,  2.2478e-01,\n        -3.5340e-01,  2.9402e+00,  5.0266e-01, -1.1648e-01, -3.3925e-01,\n        -1.4641e-01,  1.3185e-01,  5.6856e-01, -2.4193e-01,  2.3383e-01,\n        -6.1204e-01, -1.6270e-01,  2.6729e-01,  1.0813e-01, -3.3550e+00,\n         3.8122e-01,  3.1480e-01,  3.6961e-01, -1.1843e-01, -4.1384e-02,\n         2.7716e-01, -6.5405e-01, -6.8845e-01,  1.7058e-01,  2.3255e-01,\n         2.1136e-02, -2.7776e-01,  1.7016e-01,  3.5261e-01,  6.2866e-01,\n        -2.2970e-01,  5.0996e+01, -3.4832e-01, -4.9283e-01,  1.6009e-01,\n        -2.5912e-01, -4.3954e-02, -6.6652e-02,  1.7371e-02, -4.9989e-02,\n         7.7331e-02, -2.0900e-01, -5.2491e-01, -9.1295e-01, -7.5307e-02,\n        -5.9557e-02, -3.5249e-01,  9.1986e-02, -4.4967e-02,  1.7474e-01,\n         1.0622e+00, -3.1931e-02, -2.3144e-01,  1.4521e-01, -4.0620e-01,\n        -5.8457e-01,  1.4042e-02,  1.0694e-01, -1.2151e-01, -1.1069e+00,\n         4.4808e-01, -1.6879e-01,  4.8419e-02, -1.2701e+00,  1.9134e-01,\n        -7.3997e-01, -6.9388e-01,  9.3711e-03, -3.0020e-01, -2.9383e-02,\n         1.1450e-01,  2.2709e-01, -6.2942e-01,  3.4850e-01,  4.6560e-01,\n         2.9459e-01, -2.3642e-01,  4.6218e-01, -7.3631e-02,  4.3862e-01,\n         1.2286e-01,  9.8999e-01, -1.0245e+00,  2.4983e-01, -1.2601e-01,\n         4.3485e-01,  6.3421e-01,  3.8452e-01,  9.8958e-03, -6.1150e-02,\n        -2.8366e-01, -3.5584e-01, -3.4806e-01,  4.6458e-02,  1.0344e-01,\n        -1.7640e-01,  2.6008e-01,  3.0842e-01, -5.9029e-01,  6.2168e-02,\n         8.0234e-02, -2.1006e-01, -1.3377e+00,  4.7618e-01, -2.3790e-01,\n         5.6921e-01, -1.8896e-01,  8.6591e-02,  1.3231e-02,  1.2710e-01,\n        -5.3515e-01,  3.2699e-01, -2.2347e-01,  2.8715e-01, -1.1292e+00,\n         7.7704e-02,  5.1702e-01,  2.5250e-02,  8.0246e-01, -2.2748e-01,\n         4.8657e-01,  4.7917e-01,  5.0834e-01, -1.8833e-01,  5.0228e-01,\n        -3.7779e-01, -3.9931e-01,  2.4244e-01,  2.1779e-01, -1.0248e-01,\n         4.3215e-01,  1.9822e-01, -4.4591e-01,  5.6154e-01,  3.0105e-01,\n        -4.6189e-01, -8.0389e-01,  9.8432e-01, -3.1944e-01,  9.4833e-02,\n         1.1400e-01, -1.5258e-01,  8.6682e-02, -5.9506e-03,  1.0061e+00,\n         3.5015e-01, -5.4344e-01, -1.4929e-02,  2.9061e-01,  7.7716e-01,\n         5.7593e-02,  4.1433e-01,  2.0678e-01,  1.1295e-01, -3.3525e-01,\n        -6.4733e-01, -2.0853e-01, -6.8152e-01, -2.3764e-01, -7.2244e-03,\n         5.3895e-02, -1.7540e-02, -3.5193e+00, -2.7689e-01, -1.4060e-01,\n         2.2860e-01, -2.3910e-01,  1.4758e-01,  4.6928e-01, -4.1897e-01,\n        -8.1610e-01,  6.8833e-01, -5.3819e-01, -5.4749e-02, -1.4693e-01,\n         1.5520e-01,  3.4751e-01, -1.0811e-02, -1.1496e-01, -3.7802e-01,\n        -5.4832e-02, -1.7603e-01, -1.5183e-01, -3.5338e-03, -1.5314e-02,\n        -1.0327e-01, -4.9390e-01, -9.5211e-03,  2.2363e-01, -8.6061e-02,\n        -1.6430e+00,  4.8663e-01,  2.2926e-01, -1.5008e-01,  4.6846e-03,\n         1.8621e-01,  5.8974e-01, -2.3210e-01, -3.8068e-02,  5.2106e-02,\n        -2.1625e-01,  1.3113e-01, -1.6528e-01, -3.0899e-01, -8.6106e-02,\n         1.1730e-01, -2.2622e-01, -3.6722e-01, -1.5751e-01, -3.6578e-02,\n        -3.0102e-01, -4.9017e-02, -4.6044e-01,  4.7928e-01,  3.5405e-02,\n         9.5526e-02, -1.9290e-01,  3.5486e-01,  3.5565e-01, -4.3436e-01,\n        -3.7214e-01,  3.7490e-01, -1.4289e-01,  1.4892e-01,  4.4548e-01,\n        -6.1742e-02,  3.3417e-01,  1.5622e+00, -1.6550e-01, -9.5622e-02,\n        -3.1609e-01,  3.5710e-01,  1.5547e-01,  3.7913e-01, -4.5316e-01,\n        -3.0411e-01,  6.3645e-01,  4.6493e-01, -2.7021e-01,  2.8007e-01,\n         3.5020e-01,  4.3037e-01, -2.0467e-01, -1.7361e-01,  5.0707e-02,\n         5.2277e-01, -1.6470e-01,  2.9059e-01,  9.7539e-02, -8.1670e-02,\n         3.0347e-01,  3.0409e-01, -1.9271e-01, -3.8301e-01,  5.5685e-01,\n        -2.9989e-01,  7.5335e-01, -1.9442e-01, -8.1730e-01, -5.7642e-02,\n         4.5157e-01, -1.0122e+00,  3.4106e-02,  3.7177e-01,  4.8461e-01,\n         1.0395e+00,  3.5976e-01,  1.9491e-01, -1.8765e-01,  9.0858e-02,\n        -1.2970e-01, -6.9582e-02, -3.3006e-01,  1.2637e-01, -1.4938e-01,\n         1.7854e-01, -3.3438e-02,  2.9249e-01, -6.0153e-02, -3.9808e-01,\n        -4.7156e-02,  1.6853e-01, -2.4003e-02, -9.1483e-02,  3.4247e-01,\n         2.2795e-01,  3.3932e-01, -3.3874e-01, -5.3320e-02, -1.2861e-02,\n        -1.2411e-02,  3.7387e-02,  5.8254e-02, -2.5397e-01,  5.2348e-01,\n         8.6658e-01, -3.5760e-01,  1.3084e-01, -1.1010e-01,  5.1624e-01,\n         6.0349e-01, -4.8093e-01,  1.6971e-01,  1.2630e+00, -2.6303e-01,\n         1.8936e-01,  2.1825e-01,  2.9491e-02, -3.4005e-01, -4.3871e+01,\n         4.7595e-02, -1.4228e-01, -1.0846e-01, -1.5160e-01,  1.0329e-01,\n         3.7053e-01, -1.2320e-02,  3.1999e-01,  5.6906e-02,  3.0835e-01,\n         2.0784e-01, -3.3139e-01, -4.5700e-01, -1.5465e-01,  1.6962e-01,\n        -3.5235e-01, -7.5174e-03,  4.5752e-01,  4.0825e-01, -1.1673e-01,\n        -1.0488e-02,  2.5212e-02, -1.6757e-01,  2.3002e-01, -1.8673e-02,\n        -1.7697e-01, -2.1341e-01, -6.6391e-01, -1.8274e-01,  7.0124e-01,\n         2.5572e-01,  1.5160e-01, -2.0538e-01,  1.8113e-01, -4.6497e-02,\n        -2.5627e-01, -4.6529e-01,  1.9979e-01, -1.2916e-01, -3.4473e-01,\n        -4.8907e-01, -3.6988e-01,  1.0921e-01, -1.2943e-01, -1.2346e-01,\n         8.8096e-02,  1.2546e+00,  6.3094e+00, -2.5674e-01, -4.3520e-01,\n        -2.3859e+00,  4.8746e-01, -3.9125e-02, -4.1557e-02, -2.7922e-01,\n         8.7475e-02,  2.1376e-01, -6.5235e-02, -1.2277e+00, -2.0121e+01,\n         4.7794e-01,  3.3857e-01,  1.3460e-01,  4.3976e-01,  1.7539e-01,\n         2.3180e-01, -3.4298e-01,  2.6690e-01, -3.0966e-01, -8.0459e-01,\n         1.0486e-01,  2.3068e-01, -2.7034e-01, -7.2113e-02,  1.2232e+00,\n        -3.3541e-02,  1.2416e-01, -5.4055e-02,  3.2458e-01, -2.6819e-01,\n        -2.7736e-01, -1.0285e-01, -3.2138e-01,  4.5720e-01,  1.7563e-01,\n        -5.7128e-02,  1.5178e-01, -4.0476e-01,  5.8006e-02,  3.9195e-02,\n        -2.5981e-01,  2.0811e-01, -2.8853e-01,  6.6633e+00, -5.7252e-01,\n        -5.5131e-02,  7.3943e-02,  3.7546e-01,  5.8285e-02, -6.3121e-01,\n        -5.0900e-01, -5.1948e-02, -1.1226e-01, -1.6612e-01, -1.6518e-01,\n         4.8815e-01,  1.6613e-01, -4.5851e-01, -5.6316e-01, -3.2561e-02,\n        -1.4305e-01, -2.9152e-01, -6.2530e-01, -1.1286e-01,  6.0920e-03,\n         7.0946e+01,  4.8999e-01,  1.3975e-01, -1.2525e-01, -1.5065e-01,\n         1.6868e-01, -2.7682e-02, -4.1910e-01,  2.6702e-01, -1.3076e+00,\n         4.4607e-01,  1.2542e-01, -2.8018e+00,  1.4158e-01, -5.2851e-01,\n        -3.3754e-01,  1.9284e-02,  1.0982e+00,  6.1203e-02,  2.5273e-01,\n        -6.5044e-01, -1.6463e-01, -3.6856e-01,  7.3381e-02,  2.8394e-01,\n         7.1713e-01, -4.5587e-01, -3.6499e-01, -3.1927e-01,  2.5299e-01,\n         3.1554e-01, -3.9655e-01, -1.6431e-01, -3.2882e-01,  4.8321e-01,\n         2.6438e-02,  3.2431e-01, -7.3670e-01,  3.7471e-01,  2.6125e-01,\n         1.1955e-01, -2.1661e-01,  4.1328e-01,  1.6487e-01,  4.8600e-02,\n        -1.4378e-02, -7.9637e-02, -4.6863e-01, -4.1781e-01,  1.7027e+00,\n        -8.9309e-01,  1.2432e+00,  2.1844e-01,  5.7696e-02,  1.4689e-01,\n         1.5076e-01,  5.9482e-02, -6.2378e-01, -2.9110e-01, -1.6801e-01,\n        -3.6049e-01, -3.8028e-02, -2.2511e-01, -5.7139e-01, -4.8377e-01,\n        -4.3101e-02,  1.8708e+02, -2.1507e-01,  1.9170e-01, -3.0552e-02,\n         2.2827e-01,  1.0583e-01, -7.0139e-02, -2.7798e+00,  1.4771e-01,\n         8.5176e-02, -2.1687e-01,  3.3596e-01,  5.2070e-01,  1.1699e-01,\n         9.5625e-02,  7.2405e-02, -7.4354e-01, -4.1796e-01, -3.1036e-01,\n         1.9051e-01,  1.3422e-01,  1.3123e-02,  6.9384e-02,  4.9266e-01,\n        -3.0280e-01,  4.1873e-01,  3.8501e-01,  5.0008e-02, -4.1961e-01,\n        -9.2081e-01, -8.8505e-01, -4.4598e-01,  3.2327e-01,  3.3958e-01,\n         2.7451e-01,  1.5717e-02, -1.3682e-01, -1.1711e-01,  8.5470e-01,\n         2.0400e-01, -3.2440e-03, -6.6013e-01, -9.5547e-01,  1.1471e-01,\n        -2.7788e-01, -2.0668e-01,  2.4634e-01,  6.9303e-01, -4.9175e-01,\n         5.9871e-01,  2.5664e-01, -3.5525e-02, -6.6084e-01,  3.5962e-03,\n         7.7517e-01,  3.3645e-01,  4.1101e-01,  6.3882e-01, -1.1079e+00,\n         3.5268e-01, -1.6977e-01, -3.6987e-02,  2.5442e-01,  3.5130e-01,\n        -3.1887e-02, -4.1854e-01,  1.6195e-01, -1.2116e-01,  3.4556e-01,\n        -3.1018e-01, -1.4006e-01, -2.2971e-02, -2.4656e-01, -1.3791e-02,\n        -7.8256e-02,  1.3936e-01,  2.5287e-01,  1.2679e-01,  1.0444e-02,\n        -7.2022e-01,  6.0788e-02,  1.3312e-01, -5.9214e-01, -2.5595e-02,\n        -1.9300e-01,  2.0226e-01,  8.7149e-02, -2.8428e-01, -7.0516e-02,\n         2.0947e-01, -2.4657e-01, -2.4485e-01, -9.9435e-02,  3.1029e-01,\n        -3.1992e-02, -6.6761e-02, -6.7534e-01, -8.6753e-01, -6.0732e-02,\n         1.5726e-01,  7.2841e-02, -1.3594e-01,  2.1770e-01, -1.9269e-01,\n         7.0538e-02,  2.4697e-01, -2.1211e-01,  3.9387e-01, -1.1869e-01,\n         3.3987e-01, -2.5016e-01,  3.6958e-02,  1.6831e+00,  1.3615e-02,\n        -1.7321e-01,  5.6813e-01,  6.5152e-01, -1.0398e-01,  3.8231e-01,\n         3.3899e-01,  8.4053e-01,  1.7321e-01,  2.9557e-01,  4.3555e-01,\n        -3.8328e-01,  7.0625e-01,  1.4007e-01,  5.9916e-01,  3.2180e-01,\n         2.8273e-01,  5.9825e-02,  9.4940e-01, -4.5199e-02,  2.8532e-01,\n        -7.0141e-02,  5.2700e-02, -1.2425e-01, -4.9037e-02, -3.8386e-01,\n        -6.8675e-01,  5.6606e-02,  1.1794e-01, -4.0015e-01,  1.5555e-01,\n         4.9732e-01, -2.5473e-02,  6.1942e-01, -4.3914e-01,  3.6841e-01,\n        -1.0106e+00,  5.7044e-01,  6.3400e-01,  1.8109e-01, -1.2689e+00,\n         3.0568e-01, -2.4669e-01, -4.7047e-01, -6.9111e-01, -1.8980e-01,\n         9.3604e-01, -2.0410e-01, -1.1872e-01, -2.7357e-02,  2.3875e-01,\n         3.1092e-01,  1.1390e-02, -2.4784e-01, -7.8412e-03, -6.9789e-01,\n         1.5111e-01, -2.7125e-01, -2.5387e-01, -1.8569e-01,  5.4281e-01,\n        -1.2238e-01,  5.8334e-01, -1.0270e-01, -6.6581e-01,  5.0467e-02,\n         2.2914e-01, -6.2707e-01,  3.1402e-01, -2.5495e-01,  2.1849e-01,\n        -8.2420e-01,  1.0366e-01, -6.9238e-02, -1.6138e-01,  2.4286e-02,\n         8.9909e-02, -2.2955e-02,  1.1647e-01,  3.7806e-01,  3.2999e-01,\n         9.7268e-01,  2.0536e-01,  3.2675e-01,  1.2438e-01,  9.5249e-02,\n         2.0198e-01, -1.1161e-01, -1.8637e-01, -5.8571e-02, -4.2029e-01,\n        -7.9599e-02,  1.5202e-01,  2.0047e-01, -1.0811e+00, -6.1192e-01,\n        -1.4036e-01,  5.7603e-01, -1.3407e-01, -1.2577e-01, -3.3293e-01,\n        -1.4304e-01, -5.7476e-01, -3.0210e-01, -1.0654e-01,  1.8689e-01,\n        -1.1735e-01,  2.6910e-02,  3.3609e-01, -3.8913e-01,  1.3305e-02,\n        -9.7757e-01, -5.0708e-01, -3.9805e-01, -2.5544e-01,  5.2961e-01,\n         7.5593e-01,  2.7601e-01, -1.0440e+00,  4.2858e-01, -3.3402e-01,\n         4.9627e-01, -5.5644e-02,  2.6034e-01, -3.1860e-01,  1.9822e-01,\n        -6.6199e-01,  5.5858e-01, -1.4615e-01, -1.3237e-01, -4.9860e-02,\n         6.2007e-02, -1.5592e-02,  1.9362e-01, -3.9611e-01, -5.7654e-01,\n        -1.5250e+00, -1.2473e-01,  1.1547e-01,  1.7411e-02, -4.1657e-01,\n        -3.4960e-02,  3.0048e-01, -3.1938e-01,  4.4102e-01, -6.5498e-01,\n         3.7197e-01, -1.3987e-01,  1.6034e-01, -6.5139e-01,  3.1342e-01,\n        -2.8109e-01,  1.6483e-01,  1.8485e-01, -7.5067e-01,  1.2059e-01,\n         2.5470e-01, -8.5622e-02,  4.0347e-02])\n\n\nNow we animate through the residual stream.\n\nresidual_stream = np.zeros((13, d_model))\n\nfor layer in range(13):\n    residual_stream_layer = outputs.hidden_states[layer][0, -1, :].numpy()\n    residual_stream[layer] = residual_stream_layer\n    print(f\"Layer {layer}: Residual stream shape {residual_stream_layer.shape}\")\n\nLayer 0: Residual stream shape (768,)\nLayer 1: Residual stream shape (768,)\nLayer 2: Residual stream shape (768,)\nLayer 3: Residual stream shape (768,)\nLayer 4: Residual stream shape (768,)\nLayer 5: Residual stream shape (768,)\nLayer 6: Residual stream shape (768,)\nLayer 7: Residual stream shape (768,)\nLayer 8: Residual stream shape (768,)\nLayer 9: Residual stream shape (768,)\nLayer 10: Residual stream shape (768,)\nLayer 11: Residual stream shape (768,)\nLayer 12: Residual stream shape (768,)\n\n\n\nresidual_stream = rearrange(residual_stream, \"layer (row col) -&gt; layer row col\", row=16)\n\nresidual_stream.shape\n\n(13, 16, 48)\n\n\n\nfrom ipywidgets import interact, interactive, IntSlider, VBox, HBox, Output\nimport matplotlib.pyplot as plt\n\noutput = Output()\n\ndef plot_layer(layer=0):\n    with output:\n        output.clear_output(wait=True)\n        fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n        ax.axis(False)\n        fig.set_facecolor('white')\n        ax.set_facecolor('white')\n\n        cmesh = ax.pcolormesh(residual_stream[layer], edgecolors='white', cmap='viridis', vmin=-3, vmax=3)\n        ax.set_title(f'Layer {layer}', color='black', fontsize=16)\n        plt.show()\n\nslider = IntSlider(min=0, max=residual_stream.shape[0]-1, step=1, value=0, description='Layer:')\nslider.observe(lambda change: plot_layer(change['new']), names='value')\n\n# Initial plot\nplot_layer(0)\n\n# Display with slider below and centered\nfrom ipywidgets import Layout\ndisplay(VBox([output, HBox([slider], layout=Layout(justify_content='center'))],\n             layout=Layout(background_color='white')))"
  },
  {
    "objectID": "blog/posts/residual-stream/residual_stream_animation.html",
    "href": "blog/posts/residual-stream/residual_stream_animation.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\n\n\nd_model = 768\nn_layers = 12\n\nresidual_stream = np.random.randn(n_layers, d_model)\n\nresidual_stream.shape\n\n(12, 768)\n\n\nLet’s use Einops to rearrange this into a nice array to visualize.\n\nfrom einops import rearrange, reduce, repeat\n\nresidual_stream = rearrange(residual_stream, \"layer (row col) -&gt; layer row col\", row=16)\n\nresidual_stream.shape\n\n(12, 16, 48)\n\n\nNow we do the simplest plot of such a tensor.\n\nimport matplotlib.pyplot as plt\n\nplt.imshow(residual_stream[0], cmap='viridis')\nplt.title(\"Residual Stream at Layer 0\")\nplt.show()\n\n\n\n\n\n\n\n\nWe want to make this much prettier for the blog.\n\nfig, ax = plt.subplots(1,1, figsize=(15,5))\nax.axis(False)\nfig.set_facecolor('black')\nax.set_facecolor('black')\n\ncmesh = ax.pcolormesh(residual_stream[0], edgecolors='k', cmap='viridis', vmin=-3, vmax=3)\n\n\n\n\nAnimation of the Residual Stream Across Layers\n\n\n\n\nNow we can think about animating this.\n\nfrom matplotlib.animation import FuncAnimation\n\ndef anim_function(frame_num):\n    cmesh.set_array(residual_stream[frame_num,:,:])\n    return cmesh,\n\nanim = FuncAnimation(fig, anim_function, frames=np.arange(residual_stream.shape[0]), interval=30)\n\nfrom IPython.display import HTML\nHTML(anim.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nNow lets look to do something with actual model activations!\n\nfrom huggingface_hub import login\nlogin()\n\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n\n\n\nGPT2LMHeadModel LOAD REPORT from: gpt2\nKey                  | Status     |  | \n---------------------+------------+--+-\nh.{0...11}.attn.bias | UNEXPECTED |  | \n\nNotes:\n- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n\n\n\ninputs = tokenizer(\n    \"Paris is the capital of\",\n    padding=True,\n    truncation=True,\n    return_tensors=\"pt\"\n)\n\nprint(f\"Tokenized {inputs['input_ids'].shape[0]} sequences with max length {inputs['input_ids'].shape[1]}.\")\n\nTokenized 1 sequences with max length 5.\n\n\n\nwith torch.no_grad():\n    outputs = model(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        output_hidden_states=True,\n    )\n\n\ntype(outputs.hidden_states)\n\ntuple\n\n\n\noutputs.hidden_states[12][0, -1, :]\n\ntensor([ 3.9707e-01,  6.5592e-01, -1.0789e-01, -5.1339e-01,  2.2478e-01,\n        -3.5340e-01,  2.9402e+00,  5.0266e-01, -1.1648e-01, -3.3925e-01,\n        -1.4641e-01,  1.3185e-01,  5.6856e-01, -2.4193e-01,  2.3383e-01,\n        -6.1204e-01, -1.6270e-01,  2.6729e-01,  1.0813e-01, -3.3550e+00,\n         3.8122e-01,  3.1480e-01,  3.6961e-01, -1.1843e-01, -4.1384e-02,\n         2.7716e-01, -6.5405e-01, -6.8845e-01,  1.7058e-01,  2.3255e-01,\n         2.1136e-02, -2.7776e-01,  1.7016e-01,  3.5261e-01,  6.2866e-01,\n        -2.2970e-01,  5.0996e+01, -3.4832e-01, -4.9283e-01,  1.6009e-01,\n        -2.5912e-01, -4.3954e-02, -6.6652e-02,  1.7371e-02, -4.9989e-02,\n         7.7331e-02, -2.0900e-01, -5.2491e-01, -9.1295e-01, -7.5307e-02,\n        -5.9557e-02, -3.5249e-01,  9.1986e-02, -4.4967e-02,  1.7474e-01,\n         1.0622e+00, -3.1931e-02, -2.3144e-01,  1.4521e-01, -4.0620e-01,\n        -5.8457e-01,  1.4042e-02,  1.0694e-01, -1.2151e-01, -1.1069e+00,\n         4.4808e-01, -1.6879e-01,  4.8419e-02, -1.2701e+00,  1.9134e-01,\n        -7.3997e-01, -6.9388e-01,  9.3711e-03, -3.0020e-01, -2.9383e-02,\n         1.1450e-01,  2.2709e-01, -6.2942e-01,  3.4850e-01,  4.6560e-01,\n         2.9459e-01, -2.3642e-01,  4.6218e-01, -7.3631e-02,  4.3862e-01,\n         1.2286e-01,  9.8999e-01, -1.0245e+00,  2.4983e-01, -1.2601e-01,\n         4.3485e-01,  6.3421e-01,  3.8452e-01,  9.8958e-03, -6.1150e-02,\n        -2.8366e-01, -3.5584e-01, -3.4806e-01,  4.6458e-02,  1.0344e-01,\n        -1.7640e-01,  2.6008e-01,  3.0842e-01, -5.9029e-01,  6.2168e-02,\n         8.0234e-02, -2.1006e-01, -1.3377e+00,  4.7618e-01, -2.3790e-01,\n         5.6921e-01, -1.8896e-01,  8.6591e-02,  1.3231e-02,  1.2710e-01,\n        -5.3515e-01,  3.2699e-01, -2.2347e-01,  2.8715e-01, -1.1292e+00,\n         7.7704e-02,  5.1702e-01,  2.5250e-02,  8.0246e-01, -2.2748e-01,\n         4.8657e-01,  4.7917e-01,  5.0834e-01, -1.8833e-01,  5.0228e-01,\n        -3.7779e-01, -3.9931e-01,  2.4244e-01,  2.1779e-01, -1.0248e-01,\n         4.3215e-01,  1.9822e-01, -4.4591e-01,  5.6154e-01,  3.0105e-01,\n        -4.6189e-01, -8.0389e-01,  9.8432e-01, -3.1944e-01,  9.4833e-02,\n         1.1400e-01, -1.5258e-01,  8.6682e-02, -5.9506e-03,  1.0061e+00,\n         3.5015e-01, -5.4344e-01, -1.4929e-02,  2.9061e-01,  7.7716e-01,\n         5.7593e-02,  4.1433e-01,  2.0678e-01,  1.1295e-01, -3.3525e-01,\n        -6.4733e-01, -2.0853e-01, -6.8152e-01, -2.3764e-01, -7.2244e-03,\n         5.3895e-02, -1.7540e-02, -3.5193e+00, -2.7689e-01, -1.4060e-01,\n         2.2860e-01, -2.3910e-01,  1.4758e-01,  4.6928e-01, -4.1897e-01,\n        -8.1610e-01,  6.8833e-01, -5.3819e-01, -5.4749e-02, -1.4693e-01,\n         1.5520e-01,  3.4751e-01, -1.0811e-02, -1.1496e-01, -3.7802e-01,\n        -5.4832e-02, -1.7603e-01, -1.5183e-01, -3.5338e-03, -1.5314e-02,\n        -1.0327e-01, -4.9390e-01, -9.5211e-03,  2.2363e-01, -8.6061e-02,\n        -1.6430e+00,  4.8663e-01,  2.2926e-01, -1.5008e-01,  4.6846e-03,\n         1.8621e-01,  5.8974e-01, -2.3210e-01, -3.8068e-02,  5.2106e-02,\n        -2.1625e-01,  1.3113e-01, -1.6528e-01, -3.0899e-01, -8.6106e-02,\n         1.1730e-01, -2.2622e-01, -3.6722e-01, -1.5751e-01, -3.6578e-02,\n        -3.0102e-01, -4.9017e-02, -4.6044e-01,  4.7928e-01,  3.5405e-02,\n         9.5526e-02, -1.9290e-01,  3.5486e-01,  3.5565e-01, -4.3436e-01,\n        -3.7214e-01,  3.7490e-01, -1.4289e-01,  1.4892e-01,  4.4548e-01,\n        -6.1742e-02,  3.3417e-01,  1.5622e+00, -1.6550e-01, -9.5622e-02,\n        -3.1609e-01,  3.5710e-01,  1.5547e-01,  3.7913e-01, -4.5316e-01,\n        -3.0411e-01,  6.3645e-01,  4.6493e-01, -2.7021e-01,  2.8007e-01,\n         3.5020e-01,  4.3037e-01, -2.0467e-01, -1.7361e-01,  5.0707e-02,\n         5.2277e-01, -1.6470e-01,  2.9059e-01,  9.7539e-02, -8.1670e-02,\n         3.0347e-01,  3.0409e-01, -1.9271e-01, -3.8301e-01,  5.5685e-01,\n        -2.9989e-01,  7.5335e-01, -1.9442e-01, -8.1730e-01, -5.7642e-02,\n         4.5157e-01, -1.0122e+00,  3.4106e-02,  3.7177e-01,  4.8461e-01,\n         1.0395e+00,  3.5976e-01,  1.9491e-01, -1.8765e-01,  9.0858e-02,\n        -1.2970e-01, -6.9582e-02, -3.3006e-01,  1.2637e-01, -1.4938e-01,\n         1.7854e-01, -3.3438e-02,  2.9249e-01, -6.0153e-02, -3.9808e-01,\n        -4.7156e-02,  1.6853e-01, -2.4003e-02, -9.1483e-02,  3.4247e-01,\n         2.2795e-01,  3.3932e-01, -3.3874e-01, -5.3320e-02, -1.2861e-02,\n        -1.2411e-02,  3.7387e-02,  5.8254e-02, -2.5397e-01,  5.2348e-01,\n         8.6658e-01, -3.5760e-01,  1.3084e-01, -1.1010e-01,  5.1624e-01,\n         6.0349e-01, -4.8093e-01,  1.6971e-01,  1.2630e+00, -2.6303e-01,\n         1.8936e-01,  2.1825e-01,  2.9491e-02, -3.4005e-01, -4.3871e+01,\n         4.7595e-02, -1.4228e-01, -1.0846e-01, -1.5160e-01,  1.0329e-01,\n         3.7053e-01, -1.2320e-02,  3.1999e-01,  5.6906e-02,  3.0835e-01,\n         2.0784e-01, -3.3139e-01, -4.5700e-01, -1.5465e-01,  1.6962e-01,\n        -3.5235e-01, -7.5174e-03,  4.5752e-01,  4.0825e-01, -1.1673e-01,\n        -1.0488e-02,  2.5212e-02, -1.6757e-01,  2.3002e-01, -1.8673e-02,\n        -1.7697e-01, -2.1341e-01, -6.6391e-01, -1.8274e-01,  7.0124e-01,\n         2.5572e-01,  1.5160e-01, -2.0538e-01,  1.8113e-01, -4.6497e-02,\n        -2.5627e-01, -4.6529e-01,  1.9979e-01, -1.2916e-01, -3.4473e-01,\n        -4.8907e-01, -3.6988e-01,  1.0921e-01, -1.2943e-01, -1.2346e-01,\n         8.8096e-02,  1.2546e+00,  6.3094e+00, -2.5674e-01, -4.3520e-01,\n        -2.3859e+00,  4.8746e-01, -3.9125e-02, -4.1557e-02, -2.7922e-01,\n         8.7475e-02,  2.1376e-01, -6.5235e-02, -1.2277e+00, -2.0121e+01,\n         4.7794e-01,  3.3857e-01,  1.3460e-01,  4.3976e-01,  1.7539e-01,\n         2.3180e-01, -3.4298e-01,  2.6690e-01, -3.0966e-01, -8.0459e-01,\n         1.0486e-01,  2.3068e-01, -2.7034e-01, -7.2113e-02,  1.2232e+00,\n        -3.3541e-02,  1.2416e-01, -5.4055e-02,  3.2458e-01, -2.6819e-01,\n        -2.7736e-01, -1.0285e-01, -3.2138e-01,  4.5720e-01,  1.7563e-01,\n        -5.7128e-02,  1.5178e-01, -4.0476e-01,  5.8006e-02,  3.9195e-02,\n        -2.5981e-01,  2.0811e-01, -2.8853e-01,  6.6633e+00, -5.7252e-01,\n        -5.5131e-02,  7.3943e-02,  3.7546e-01,  5.8285e-02, -6.3121e-01,\n        -5.0900e-01, -5.1948e-02, -1.1226e-01, -1.6612e-01, -1.6518e-01,\n         4.8815e-01,  1.6613e-01, -4.5851e-01, -5.6316e-01, -3.2561e-02,\n        -1.4305e-01, -2.9152e-01, -6.2530e-01, -1.1286e-01,  6.0920e-03,\n         7.0946e+01,  4.8999e-01,  1.3975e-01, -1.2525e-01, -1.5065e-01,\n         1.6868e-01, -2.7682e-02, -4.1910e-01,  2.6702e-01, -1.3076e+00,\n         4.4607e-01,  1.2542e-01, -2.8018e+00,  1.4158e-01, -5.2851e-01,\n        -3.3754e-01,  1.9284e-02,  1.0982e+00,  6.1203e-02,  2.5273e-01,\n        -6.5044e-01, -1.6463e-01, -3.6856e-01,  7.3381e-02,  2.8394e-01,\n         7.1713e-01, -4.5587e-01, -3.6499e-01, -3.1927e-01,  2.5299e-01,\n         3.1554e-01, -3.9655e-01, -1.6431e-01, -3.2882e-01,  4.8321e-01,\n         2.6438e-02,  3.2431e-01, -7.3670e-01,  3.7471e-01,  2.6125e-01,\n         1.1955e-01, -2.1661e-01,  4.1328e-01,  1.6487e-01,  4.8600e-02,\n        -1.4378e-02, -7.9637e-02, -4.6863e-01, -4.1781e-01,  1.7027e+00,\n        -8.9309e-01,  1.2432e+00,  2.1844e-01,  5.7696e-02,  1.4689e-01,\n         1.5076e-01,  5.9482e-02, -6.2378e-01, -2.9110e-01, -1.6801e-01,\n        -3.6049e-01, -3.8028e-02, -2.2511e-01, -5.7139e-01, -4.8377e-01,\n        -4.3101e-02,  1.8708e+02, -2.1507e-01,  1.9170e-01, -3.0552e-02,\n         2.2827e-01,  1.0583e-01, -7.0139e-02, -2.7798e+00,  1.4771e-01,\n         8.5176e-02, -2.1687e-01,  3.3596e-01,  5.2070e-01,  1.1699e-01,\n         9.5625e-02,  7.2405e-02, -7.4354e-01, -4.1796e-01, -3.1036e-01,\n         1.9051e-01,  1.3422e-01,  1.3123e-02,  6.9384e-02,  4.9266e-01,\n        -3.0280e-01,  4.1873e-01,  3.8501e-01,  5.0008e-02, -4.1961e-01,\n        -9.2081e-01, -8.8505e-01, -4.4598e-01,  3.2327e-01,  3.3958e-01,\n         2.7451e-01,  1.5717e-02, -1.3682e-01, -1.1711e-01,  8.5470e-01,\n         2.0400e-01, -3.2440e-03, -6.6013e-01, -9.5547e-01,  1.1471e-01,\n        -2.7788e-01, -2.0668e-01,  2.4634e-01,  6.9303e-01, -4.9175e-01,\n         5.9871e-01,  2.5664e-01, -3.5525e-02, -6.6084e-01,  3.5962e-03,\n         7.7517e-01,  3.3645e-01,  4.1101e-01,  6.3882e-01, -1.1079e+00,\n         3.5268e-01, -1.6977e-01, -3.6987e-02,  2.5442e-01,  3.5130e-01,\n        -3.1887e-02, -4.1854e-01,  1.6195e-01, -1.2116e-01,  3.4556e-01,\n        -3.1018e-01, -1.4006e-01, -2.2971e-02, -2.4656e-01, -1.3791e-02,\n        -7.8256e-02,  1.3936e-01,  2.5287e-01,  1.2679e-01,  1.0444e-02,\n        -7.2022e-01,  6.0788e-02,  1.3312e-01, -5.9214e-01, -2.5595e-02,\n        -1.9300e-01,  2.0226e-01,  8.7149e-02, -2.8428e-01, -7.0516e-02,\n         2.0947e-01, -2.4657e-01, -2.4485e-01, -9.9435e-02,  3.1029e-01,\n        -3.1992e-02, -6.6761e-02, -6.7534e-01, -8.6753e-01, -6.0732e-02,\n         1.5726e-01,  7.2841e-02, -1.3594e-01,  2.1770e-01, -1.9269e-01,\n         7.0538e-02,  2.4697e-01, -2.1211e-01,  3.9387e-01, -1.1869e-01,\n         3.3987e-01, -2.5016e-01,  3.6958e-02,  1.6831e+00,  1.3615e-02,\n        -1.7321e-01,  5.6813e-01,  6.5152e-01, -1.0398e-01,  3.8231e-01,\n         3.3899e-01,  8.4053e-01,  1.7321e-01,  2.9557e-01,  4.3555e-01,\n        -3.8328e-01,  7.0625e-01,  1.4007e-01,  5.9916e-01,  3.2180e-01,\n         2.8273e-01,  5.9825e-02,  9.4940e-01, -4.5199e-02,  2.8532e-01,\n        -7.0141e-02,  5.2700e-02, -1.2425e-01, -4.9037e-02, -3.8386e-01,\n        -6.8675e-01,  5.6606e-02,  1.1794e-01, -4.0015e-01,  1.5555e-01,\n         4.9732e-01, -2.5473e-02,  6.1942e-01, -4.3914e-01,  3.6841e-01,\n        -1.0106e+00,  5.7044e-01,  6.3400e-01,  1.8109e-01, -1.2689e+00,\n         3.0568e-01, -2.4669e-01, -4.7047e-01, -6.9111e-01, -1.8980e-01,\n         9.3604e-01, -2.0410e-01, -1.1872e-01, -2.7357e-02,  2.3875e-01,\n         3.1092e-01,  1.1390e-02, -2.4784e-01, -7.8412e-03, -6.9789e-01,\n         1.5111e-01, -2.7125e-01, -2.5387e-01, -1.8569e-01,  5.4281e-01,\n        -1.2238e-01,  5.8334e-01, -1.0270e-01, -6.6581e-01,  5.0467e-02,\n         2.2914e-01, -6.2707e-01,  3.1402e-01, -2.5495e-01,  2.1849e-01,\n        -8.2420e-01,  1.0366e-01, -6.9238e-02, -1.6138e-01,  2.4286e-02,\n         8.9909e-02, -2.2955e-02,  1.1647e-01,  3.7806e-01,  3.2999e-01,\n         9.7268e-01,  2.0536e-01,  3.2675e-01,  1.2438e-01,  9.5249e-02,\n         2.0198e-01, -1.1161e-01, -1.8637e-01, -5.8571e-02, -4.2029e-01,\n        -7.9599e-02,  1.5202e-01,  2.0047e-01, -1.0811e+00, -6.1192e-01,\n        -1.4036e-01,  5.7603e-01, -1.3407e-01, -1.2577e-01, -3.3293e-01,\n        -1.4304e-01, -5.7476e-01, -3.0210e-01, -1.0654e-01,  1.8689e-01,\n        -1.1735e-01,  2.6910e-02,  3.3609e-01, -3.8913e-01,  1.3305e-02,\n        -9.7757e-01, -5.0708e-01, -3.9805e-01, -2.5544e-01,  5.2961e-01,\n         7.5593e-01,  2.7601e-01, -1.0440e+00,  4.2858e-01, -3.3402e-01,\n         4.9627e-01, -5.5644e-02,  2.6034e-01, -3.1860e-01,  1.9822e-01,\n        -6.6199e-01,  5.5858e-01, -1.4615e-01, -1.3237e-01, -4.9860e-02,\n         6.2007e-02, -1.5592e-02,  1.9362e-01, -3.9611e-01, -5.7654e-01,\n        -1.5250e+00, -1.2473e-01,  1.1547e-01,  1.7411e-02, -4.1657e-01,\n        -3.4960e-02,  3.0048e-01, -3.1938e-01,  4.4102e-01, -6.5498e-01,\n         3.7197e-01, -1.3987e-01,  1.6034e-01, -6.5139e-01,  3.1342e-01,\n        -2.8109e-01,  1.6483e-01,  1.8485e-01, -7.5067e-01,  1.2059e-01,\n         2.5470e-01, -8.5622e-02,  4.0347e-02])\n\n\nNow we animate through the residual stream.\n\nresidual_stream = np.zeros((13, d_model))\n\nfor layer in range(13):\n    residual_stream_layer = outputs.hidden_states[layer][0, -1, :].numpy()\n    residual_stream[layer] = residual_stream_layer\n    print(f\"Layer {layer}: Residual stream shape {residual_stream_layer.shape}\")\n\nLayer 0: Residual stream shape (768,)\nLayer 1: Residual stream shape (768,)\nLayer 2: Residual stream shape (768,)\nLayer 3: Residual stream shape (768,)\nLayer 4: Residual stream shape (768,)\nLayer 5: Residual stream shape (768,)\nLayer 6: Residual stream shape (768,)\nLayer 7: Residual stream shape (768,)\nLayer 8: Residual stream shape (768,)\nLayer 9: Residual stream shape (768,)\nLayer 10: Residual stream shape (768,)\nLayer 11: Residual stream shape (768,)\nLayer 12: Residual stream shape (768,)\n\n\n\nresidual_stream = rearrange(residual_stream, \"layer (row col) -&gt; layer row col\", row=16)\n\nresidual_stream.shape\n\n(13, 16, 48)\n\n\n\nfrom ipywidgets import interact, interactive, IntSlider, VBox, HBox, Output\nimport matplotlib.pyplot as plt\n\noutput = Output()\n\ndef plot_layer(layer=0):\n    with output:\n        output.clear_output(wait=True)\n        fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n        ax.axis(False)\n        fig.set_facecolor('black')\n        ax.set_facecolor('black')\n\n        cmesh = ax.pcolormesh(residual_stream[layer], edgecolors='black', cmap='viridis', vmin=-3, vmax=3)\n        ax.set_title(f'Layer {layer}', color='white', fontsize=16)\n        plt.show()\n\nslider = IntSlider(min=0, max=residual_stream.shape[0]-1, step=1, value=0, description='Layer:')\nslider.observe(lambda change: plot_layer(change['new']), names='value')\n\n# Initial plot\nplot_layer(0)\n\n# Display with slider below and centered\nfrom ipywidgets import Layout\ndisplay(VBox([output, HBox([slider], layout=Layout(justify_content='center'))],\n             layout=Layout(background_color='white')))"
  },
  {
    "objectID": "blog/posts/residual-stream/logit-lens.html",
    "href": "blog/posts/residual-stream/logit-lens.html",
    "title": "",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nfrom matplotlib.colors import LogNorm\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\n# Clear MPS cache\nif torch.backends.mps.is_available():\n    torch.mps.empty_cache()\n\n# Also clear any cached gradients\nimport gc\ngc.collect()\n\n35\n\n\n\ndevice = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nmodel_name = \"microsoft/phi-3-mini-4k-instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name).to(device)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nmodel\n\n\n\n\nPhi3ForCausalLM(\n  (model): Phi3Model(\n    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n    (layers): ModuleList(\n      (0-31): 32 x Phi3DecoderLayer(\n        (self_attn): Phi3Attention(\n          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n        )\n        (mlp): Phi3MLP(\n          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n          (activation_fn): SiLUActivation()\n        )\n        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n    (rotary_emb): Phi3RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n)\n\n\n\ntext = \"Citron: Acide\\nSucre: Sucré\\nPiment: Épicé\\nMiel:\"\ntext_tokenized = tokenizer.encode(text, return_tensors=\"pt\").to(device)\noutput = model.generate(text_tokenized, num_beams=4, max_new_tokens=3, do_sample=True)\ntext_decoded = tokenizer.decode(output[0])\nprint(text_decoded)\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n\n\nCitron: Acide\nSucre: Sucré\nPiment: Épicé\nMiel: Doux\n\n\n\n\ndef decode_token_with_logitlens(model, device, tokenizer, input, tokens_to_gen=None):\n    '''\n    outputs a dictionary with {'decoded_tokens': [num_layers, seq_len], 'decoded_logits': [num_layers, seq_len]}\n    '''\n    inputs = tokenizer(input, return_tensors=\"pt\").to(device)\n\n    text = tokenizer.decode(inputs['input_ids'][0])\n    # run the loop to generate new tokens after input, append to input and decode\n    if tokens_to_gen != None:\n        # generate new tokens all at once; then append them to input, then logitlens them all\n        output = model.generate(\n            inputs['input_ids'],\n            do_sample=True,\n            top_p=0.95,\n            temperature=0.001,\n            top_k=0,\n            max_new_tokens=tokens_to_gen,\n            )\n        new_token = tokenizer.decode(output[0][-tokens_to_gen:])\n        text += new_token\n        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    text_tokens = [tokenizer.decode(id) for id in inputs['input_ids'][0]]\n\n    # apply decoder lens\n    classifier_head = model.lm_head # Linear(in_features=3072, out_features=32064, bias=False)\n\n    hidden_states = model(**inputs, output_hidden_states = True).hidden_states\n    decoded_intermediate_token = {}\n    decoded_intermediate_logit = {}\n    with torch.no_grad():\n        for layer_id in range(len(hidden_states)):\n            hidden_state = hidden_states[layer_id]\n            decoded_value = classifier_head(hidden_state) # [batch, seq_len, vocab_size]\n            # get probabilities\n            decoded_values = torch.nn.functional.softmax(decoded_value, dim=-1)\n            # take max element\n            argmax = torch.argmax(decoded_values, dim=-1)[0] # select first element in batch\n            # decode all tokens\n            decoded_token = [tokenizer.decode(int(el)) for el in argmax]\n            decoded_logit = [decoded_values[0, it, argmax[it]].item() for it in range(len(argmax))] # list of layers, per layer the sequence_length\n            decoded_intermediate_token[layer_id] = decoded_token\n            decoded_intermediate_logit[layer_id] = decoded_logit\n\n    tokens = list(decoded_intermediate_token.values()) # [num_layers, seq_len]\n    logits = list(decoded_intermediate_logit.values()) # [num_layers, seq_len]\n    return {'text_tokens':text_tokens, 'decoded_tokens': tokens, 'decoded_logits': logits}\n\n\ndict_output = decode_token_with_logitlens(model, device, tokenizer, text, tokens_to_gen=2)\ndecoded_tokens = dict_output['decoded_tokens']\ndecoded_logits = dict_output['decoded_logits']\ntext_tokens = dict_output['text_tokens']\nprint(len(text_tokens), text_tokens)\n\n25 ['Cit', 'ron', ':', 'A', 'cide', '\\n', 'Su', 'cre', ':', 'S', 'uc', 'ré', '\\n', 'P', 'iment', ':', 'É', 'pic', 'é', '\\n', 'M', 'iel', ':', 'D', 'oux']\n\n\n\nfor element in range(len(decoded_tokens)):\n    print(decoded_tokens[element])\n\n['izen', 'unci', 'Screen', 'bove', 'zat', 'Datos', 'ites', 'opening', 'Screen', 'orted', 'Stra', 'imas', 'Datos', 'rior', 'Db', 'Screen', 'cze', 'éd', 'parc', 'Datos', 'arch', 'merge', 'Screen', 'enis', 'Terr']\n['míst', '☺', 'míst', 'míst', '🌍', 'míst', 'becom', 'míst', 'míst', '↵', '\\x97', '̶', 'míst', '↵', 'hbox', 'míst', '̶', '\\x97', '☺', 'míst', '↵', 'Ê', 'míst', '♯', 'Č']\n['míst', '☺', 'míst', 'míst', '￼', 'míst', 'cí', 'míst', 'míst', 'míst', '›', '̶', 'míst', 'míst', 'ão', 'míst', '̶', '�', '☺', 'míst', 'cí', 'Vé', '─', '♯', 'Č']\n['option', 'Niem', '\\x97', 'mí', 'has', '\\x97', '马', 'pick', 'infatti', '⇔', '›', 'men', '\\x97', 'OL', 'wind', '\\x97', '⇔', 'Sver', 'living', '\\x97', '↵', 'Vé', '\\x97', 'Martí', 'Č']\n['option', 'ouc', 'highly', 'ero', 'practices', '\\x97', 'ivi', 'int', 'infatti', 'míst', 'lande', 'ợ', '￼', 'OL', 'al', '\\x97', '⟶', 'nic', 'surr', '\\x97', '⇔', 'rig', '\\x97', 'LT', 'fil']\n['option', 'r', 'Fich', 'mí', 'ity', '￼', '⟶', 'at', 'Fich', 'echter', 'uro', 'exclus', '￼', 'urg', 'tab', 'Fon', '🌍', 'nick', 'bean', '￼', 'cí', 'rig', 'Fon', 'Martí', 'ﬁ']\n['option', 'ville', 'alberga', 'dynamically', 'ogen', '￼', '♦', 'abet', 'ば', 'míst', '☺', 'ones', '￼', 'cí', 'anu', '⇔', 'Err', 'urg', 'itos', '￼', 'cí', 'rig', 'prima', 'Martí', 'brow']\n['option', '\\u2009', '⇔', 'ero', 'gra', '￼', 'pon', 'LES', 'cogn', '⇔', '☺', 'ば', '￼', 'cí', 'ão', '⇔', 'Err', 'ere', '➖', '￼', 'penas', 'pan', 'prima', '\\x97', 'brow']\n['option', '\\u2009', '⇔', 'dynamically', 'ity', '✔', 'cre', 'ville', 'prima', 'OL', 'loc', 'emer', '￼', 'urg', 'on', 'prima', 'cot', 'ere', 'acon', '￼', 'urg', 'de', 'prima', '↔', '-']\n['option', 'isch', '{\\r', 'oret', 'de', '↔', 'cre', 'de', 'prima', 'urg', 'loc', 'taste', 'ext', 'OL', 'on', 'cons', 'aster', 'ere', 'ol', 'ext', 'urg', 'de', 'cons', 'our', '-']\n['option', 'soft', '{\\r', 'war', 'de', '✅', 'pon', 'de', 'pre', 'urg', 'loc', ']],', 'odor', 'FA', 'on', 'pre', 'cot', 'ere', 'flav', 'soft', 'urg', 'de', 'prima', '↔', '-']\n['option', 'iar', '{\\r', 'результате', 'de', 'con', 'oc', 'de', 'pre', 'urg', 'ita', 'exc', 'soft', 'ü', 'on', 'pre', 'oc', 'результате', 'ien', 'soft', 'urg', 'de', 'op', 'ign', '-']\n['option', 'iar', '{\\r', 'war', 'de', '});\\r', 'qu', 'de', 'pre', 'eng', 'е', 'sens', 'soft', 'ü', 'on', 'pre', 'ol', 'ere', 'ien', 'soft', 'ü', 'eli', 'prom', 'lam', '-']\n['option', 'iar', '{\\r', 'ali', 'ity', '});\\r', 'con', 'de', 'prom', 'eng', 'ill', 'Tout', 'soft', 'ü', 'on', 'rig', 'lam', 'ere', 'flav', 'etc', 'adr', 'eli', 'prom', 'riv', '-']\n['option', 'ica', '{\\r', 'ali', 'de', '{\\r', 'im', 'de', 'li', 'urg', 'ub', 'de', 'adr', 'urg', 'on', 'sens', 'lam', 'ere', '/', 'etc', 'urg', 'eli', 'sens', 'ill', '-']\n['option', 'ica', '{\\r', 'ali', 'ity', '{\\r', 'ver', 'de', 'li', 'urg', 'ub', 'ais', '\\ufeff', 'urg', 'on', 'sens', 'vol', 'ere', 'option', 'etc', 'ast', 'loc', 'war', 'urg', '-']\n['option', 'ica', '{\\r', 'myster', 'ity', '{\\r', 'ft', 'de', 'ase', 'urg', 'amer', ']]', '{`', 'urg', 'on', 'sens', 'lam', 'ere', 'option', 'etc', 'ast', 'loc', 'sens', 'riv', '-']\n['option', 'ica', '{\\r', 'lem', 'de', '});\\r', 'pect', 'de', 'sens', 'urg', 'ub', '\\ufeff', 'etc', 'urg', 'on', 'sens', 'vol', 'ere', '/', 'etc', 'ast', 'loc', 'sens', 'riv', '-']\n['option', 'ic', '{\\r', 'lem', 'de', '});\\r', 'ft', 'de', 'sens', 'ill', 'ub', '\"}', '{`', 'urg', 'al', 'sens', 'vol', 'enter', 'taste', 'etc', 'ast', 'loc', 'sens', 'riv', 'soft']\n['option', 'et', '{\\r', 'modern', 'de', '&lt;|endoftext|&gt;', 'il', 'de', '', 'our', 'ub', '/', '{`', 'urg', 'al', 'sens', 'vol', 'enter', '/', '&lt;|endoftext|&gt;', 'ast', 's', 'sens', 'urg', '-']\n['option', 'et', '{\\r', 'compreh', 'de', '          ', 'il', 'de', 'b', 'our', 'ub', '\\n', 'etc', 'urg', 'al', 'sens', 'vol', 'enter', '/', 'etc', 'our', ':', 'sens', 'il', '-']\n['option', 'et', '{\\r', 'compreh', 'de', '{\\r', 'ven', 'de', '', 'our', 'our', '\\n', 'etc', 'eg', 'al', 'sens', 'vol', 'enter', '&lt;|endoftext|&gt;', 'etc', 'ur', ':', 'de', 'ur', '\\n']\n['option', 'et', '{\\r', 'Sidenote', 'de', '\\t', 'ffic', 'de', '', 'uc', 'ub', '\\n', 't', 'eg', 'al', 's', 'vol', 'enter', '\\n', '&lt;|endoftext|&gt;', 'ast', ':', 'sweet', 'iffer', '\\n']\n['option', 'et', '\\r', 'Sidenote', 'de', '✅', 'plement', ':', '', 'que', 'urs', '\\n', 't', 'eg', 'o', 'd', 'vol', 'enter', '/', 'le', 'ast', ':', 'sweet', 'iffer', '\\n']\n['option', 'et', '\"', 'Sidenote', 'ac', '✅', 'plement', ':', '', 'uc', 'rose', '\\n', '', 'ast', 'ary', 'bitter', 'vol', 'enter', '\\n', '&lt;|endoftext|&gt;', 'ast', ':', 'sweet', 'ivid', '\\n']\n['option', 'et', '\"', 'Sidenote', 'le', '', 'plement', ':', 'sugar', 'weet', 'rose', '\\n', 'gr', 'ast', 'de', 'bitter', 'vol', 'enter', '\\n', 'c', 'our', ':', 'sweet', 'ivid', '\\n']\n['option', 'et', '', 'Sidenote', 'ac', '', 'plement', ':', 'Sug', 'weet', 'rose', '\\n', 'c', 'ig', ':', 'hot', 'vol', 'enter', '\\n', 'c', 'ature', ':', 'sweet', 'ivid', '\\n']\n['option', 'et', 'Sidenote', 'Sidenote', 'ac', '', 'plement', ':', 'sol', 'weet', 'rose', '\\n', '', 'iment', ':', 'sp', 'colog', 'enter', '\\n', 'D', 'astic', ':', 'sweet', 'ul', '\\n']\n['option', 'et', 'Sidenote', 'Sidenote', 's', '', 'plement', ':', '', 'uc', 'rose', '\\n', '', 'iment', ':', 'sp', 'per', 'et', '\\n', 'po', 'astic', ':', 'swe', 'ul', '\\n']\n['option', 'et', '', 'Sidenote', '', '', 'cc', ':', '', 'atur', 'id', '\\n', '', 'iment', ':', '', 'per', 'ur', '\\n', '', 'int', ':', 'D', 'ess', '\\n']\n['Ł', 'et', '', 'Sidenote', '', '\\n', 'cre', ':', '', 'uc', 'rose', '\\n', '', 'iment', ':', '', 'per', 'ur', '\\n', 'A', 'ent', ':', 'D', 'u', '\\n']\n['\\u202f', 'i', '', '', '', '\\n', 'it', ':', '', 'uc', 'rose', '\\n', '\\n', 'ain', ':', 'A', 'p', 'ur', '\\n', '\\n', 'ent', ':', 'D', 'u', '\\n']\n['izen', 'ella', '\\n', 'Cit', 'cit', '\\n', 'ivi', ':', 'Gl', 'uc', 'ré', '\\n', '\\n', 'om', ':', 'A', 'pic', 'é', '\\n', 'C', 'iel', ':', 'D', 'oux', '\\n']\n\n\n\ntokens_viz = [tok for tok in decoded_tokens] # [num_layers, num_tokens]\nlogits_viz = np.array([log for log in decoded_logits]) # [num_layers, num_tokens]\na = [it for it in range(len(text_tokens))]\nb = [tok for tok in text_tokens]\ncol_labels = [str(a_)+': '+b_ for a_, b_ in zip(a, b)]\nnorm = plt.Normalize(logits_viz.min()-1, logits_viz.max()+1)\ncolours = plt.cm.cool(norm(logits_viz))\nfig, ax = plt.subplots(figsize=(20,5), dpi=1000)\n# hide axes\nfig.patch.set_visible(False)\nax.axis('off')\nax.axis('tight')\nimg = plt.imshow(norm(logits_viz), cmap=\"cool\")\nplt.colorbar()\nimg.set_visible(False)\nax.table(cellText=tokens_viz, rowLabels=[lay for lay in range(len(decoded_tokens))], colLabels=col_labels, colWidths = [0.2]*logits_viz.shape[1], loc='center', cellColours=img.to_rgba(norm(logits_viz)))\nplt.show()\n\n/Users/fletcaw1/Documents/Personal/personal-repos/andrewstephenfletcher.github.io/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 127757 (\\N{EARTH GLOBE EUROPE-AFRICA}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/Users/fletcaw1/Documents/Personal/personal-repos/andrewstephenfletcher.github.io/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 151 (\\x97) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/Users/fletcaw1/Documents/Personal/personal-repos/andrewstephenfletcher.github.io/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 39532 (\\N{CJK UNIFIED IDEOGRAPH-9A6C}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/Users/fletcaw1/Documents/Personal/personal-repos/andrewstephenfletcher.github.io/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 12400 (\\N{HIRAGANA LETTER BA}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/Users/fletcaw1/Documents/Personal/personal-repos/andrewstephenfletcher.github.io/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 10134 (\\N{HEAVY MINUS SIGN}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n) missing from font(s) DejaVu Sans.personal-repos/andrewstephenfletcher.github.io/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 13 (\n  fig.canvas.print_figure(bytes_io, **kw)\n/Users/fletcaw1/Documents/Personal/personal-repos/andrewstephenfletcher.github.io/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 9989 (\\N{WHITE HEAVY CHECK MARK}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/Users/fletcaw1/Documents/Personal/personal-repos/andrewstephenfletcher.github.io/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 9 ( ) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n\n\n\n\n# Find where \"Miel: Doux\" tokens start\nsearch_text = \"Miel: Doux\"\nfull_text = ''.join(text_tokens)\n\n# Find the token indices for \"Miel: Doux\"\n# First, let's see which tokens contain these characters\nmiel_start_idx = None\nfor i, token in enumerate(text_tokens):\n    if \"Miel\" in token:\n        miel_start_idx = i\n        break\n\n# Print to see what we found\nprint(f\"Full text tokens: {text_tokens}\")\nprint(f\"Miel starts at token index: {miel_start_idx}\")\n\n# Let's look at a few tokens around \"Miel:\"\nif miel_start_idx is not None:\n    # Show tokens from Miel onwards (adjust range as needed)\n    to_viz = (miel_start_idx, min(miel_start_idx + 5, len(text_tokens)))\nelse:\n    # Fallback: show last few tokens if we can't find \"Miel\"\n    to_viz = (max(0, len(text_tokens) - 5), len(text_tokens))\n\nprint(f\"Visualizing tokens {to_viz[0]} to {to_viz[1]}: {text_tokens[to_viz[0]:to_viz[1]]}\")\n\ntokens_viz = [tok[to_viz[0]:to_viz[1]] for tok in decoded_tokens]\nlogits_viz = np.array([logit[to_viz[0]:to_viz[1]] for logit in decoded_logits])\na = [it for it in range(to_viz[0], to_viz[1])]\nb = [tok for tok in text_tokens[to_viz[0]:to_viz[1]]]\ncol_labels = [str(a_)+': '+b_ for a_, b_ in zip(a, b)]\nnorm = plt.Normalize(logits_viz.min()-1, logits_viz.max()+1)\ncolours = plt.cm.cool(norm(logits_viz))\nfig, ax = plt.subplots(figsize=(20,5), dpi=1000)\nfig.patch.set_visible(False)\nax.axis('off')\nax.axis('tight')\nimg = plt.imshow(norm(logits_viz), cmap=\"cool\")\nplt.colorbar()\nimg.set_visible(False)\nax.table(cellText=tokens_viz, rowLabels=[lay for lay in range(len(decoded_tokens))], colLabels=col_labels, colWidths = [0.2]*logits_viz.shape[1], loc='center', cellColours=img.to_rgba(norm(logits_viz)))\nplt.show()\n\nFull text tokens: ['Cit', 'ron', ':', 'A', 'cide', '\\n', 'Su', 'cre', ':', 'S', 'uc', 'ré', '\\n', 'P', 'iment', ':', 'É', 'pic', 'é', '\\n', 'M', 'iel', ':', 'D', 'oux']\nMiel starts at token index: None\nVisualizing tokens 20 to 25: ['M', 'iel', ':', 'D', 'oux']\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndef plot_pretty_logit_lens(tokens_viz, logits_viz, text_tokens, start_idx, end_idx, start_layer=0):\n    \"\"\"\n    Creates a professional Logit Lens visualization.\n    \"\"\"\n    # Filter to only show layers from start_layer onwards\n    tokens_viz = tokens_viz[start_layer:]\n    logits_viz = logits_viz[start_layer:]\n\n    # 1. Setup Data Labels\n    # Rows: Layers, Columns: Pos + Token\n    row_labels = [f\"Layer {i}\" for i in range(start_layer, start_layer + len(tokens_viz))]\n    col_labels = [f\"{i}: {text_tokens[i]}\" for i in range(start_idx, end_idx)]\n\n    # 2. Styling Parameters\n    plt.figure(figsize=(max(12, len(col_labels)*2), len(row_labels)*0.6))\n    sns.set_theme(style=\"white\")\n\n    # 3. Create Heatmap\n    # We use logits_viz for the colors and tokens_viz for the text annotations\n    ax = sns.heatmap(\n        logits_viz,\n        annot=tokens_viz,\n        fmt=\"\",                   # We are passing strings, so no formatting\n        cmap=\"mako\",              # Standard, readable colormap (try 'magma' or 'viridis' too)\n        linewidths=0.5,\n        linecolor='white',\n        cbar_kws={'label': 'Logit Magnitude'},\n        xticklabels=col_labels,\n        yticklabels=row_labels,\n        annot_kws={\"size\": 10, \"weight\": \"bold\"}\n    )\n\n    # 4. Refine Labels\n    ax.xaxis.tick_top() # Move tokens to the top for easier reading\n    ax.xaxis.set_label_position('top')\n    plt.xticks(rotation=45, ha='left')\n    plt.yticks(rotation=0)\n\n    plt.title(\"Logit Lens: Evolution of Token Predictions\", pad=40, fontsize=16)\n    plt.tight_layout()\n\n    plt.savefig(\n        \"images/logit_lens_viz.svg\",\n        format='svg',\n        bbox_inches='tight',\n        transparent=False,\n        metadata={'Creator': 'Logit Lens Visualizer'}\n    )\n\n    plt.show()\n\n# --- Integration with your existing variables ---\n# Extract the slice of data needed\ns_idx, e_idx = to_viz\nfinal_tokens = [lay[s_idx:e_idx] for lay in decoded_tokens]\nfinal_logits = np.array([logit[s_idx:e_idx] for logit in decoded_logits])\n\n# Call with start_layer=12 to begin at Layer 12\nplot_pretty_logit_lens(final_tokens, final_logits, text_tokens, s_idx, e_idx, start_layer=12)\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport torch\n\n\ninputs = tokenizer(text, return_tensors=\"pt\").to(device)\n\n# Get the hidden state at layer 24, token position 22\nlayer_24_hidden = model(**tokenizer(tokenizer.decode(inputs['input_ids'][0]),\n                                    return_tensors=\"pt\").to(device),\n                                    output_hidden_states=True).hidden_states[22]\n\n# Apply classifier head to get logits for all vocabulary\nwith torch.no_grad():\n    logits = model.lm_head(layer_24_hidden[0, 22, :])  # [vocab_size]\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n\n    # Get top 10\n    top_10_probs, top_10_indices = torch.topk(probs, 10)\n\n    # Decode tokens\n    top_10_tokens = [tokenizer.decode(int(idx)) for idx in top_10_indices]\n    top_10_probs = top_10_probs.float().cpu().numpy()  # Convert to float32 first\n\n# Reverse for display (highest at top)\ntop_10_tokens = top_10_tokens[::-1]\ntop_10_probs = top_10_probs[::-1]\n\n# Create horizontal bar chart\nfig, ax = plt.subplots(figsize=(10, 6))\n\ny_positions = range(len(top_10_tokens))\nax.barh(y_positions, top_10_probs, color='steelblue', alpha=0.7)\n\nax.set_yticks(y_positions)\nax.set_yticklabels([f\"{token}\" for token in top_10_tokens])\nax.set_xlabel('Softmax Probability')\nax.set_title(f'Layer 24, Token Position 22 (\"{text_tokens[22]}\") - Top 10 Predictions')\nax.set_xlim(0, max(top_10_probs) * 1.1)\n\nax.grid(axis='x', alpha=0.3)\n\nfor i, prob in enumerate(top_10_probs):\n    ax.text(prob + 0.001, i, f'{prob:.4f}', va='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport torch\n\n\ninputs = tokenizer(text, return_tensors=\"pt\").to(device)\n\n# Get the hidden state at layer 24, token position 22\nlayer_24_hidden = model(**tokenizer(tokenizer.decode(inputs['input_ids'][0]),\n                                    return_tensors=\"pt\").to(device),\n                                    output_hidden_states=True).hidden_states[22]\n\n# Apply classifier head to get logits for all vocabulary\nwith torch.no_grad():\n    logits = model.lm_head(layer_24_hidden[0, 22, :])  # [vocab_size]\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n\n    # Get top 10\n    top_10_probs, top_10_indices = torch.topk(probs, 10)\n\n    # Get corresponding logits\n    top_10_logits = logits[top_10_indices]\n\n    # Decode tokens\n    top_10_tokens = [tokenizer.decode(int(idx)) for idx in top_10_indices]\n    top_10_probs = top_10_probs.float().cpu().numpy()\n    top_10_logits = top_10_logits.float().cpu().numpy()\n\n# Reverse for display (highest at top)\ntop_10_tokens = top_10_tokens[::-1]\ntop_10_probs = top_10_probs[::-1]\ntop_10_logits = top_10_logits[::-1]\n\n# Create horizontal bar chart\nfig, ax = plt.subplots(figsize=(10, 6))\n\ny_positions = range(len(top_10_tokens))\nax.barh(y_positions, top_10_probs, color='steelblue', alpha=0.7)\n\nax.set_yticks(y_positions)\nax.set_yticklabels([f\"{token}\" for token in top_10_tokens])\nax.set_xlabel('Softmax Probability')\nax.set_title(f'Layer 24, Token Position 22 (\"{text_tokens[22]}\") - Top 10 Predictions')\nax.set_xlim(0, max(top_10_probs) * 1.1)\n\nax.grid(axis='x', alpha=0.3)\n\nfor i, (prob, logit) in enumerate(zip(top_10_probs, top_10_logits)):\n    ax.text(prob + 0.001, i, f'{prob:.4f} (logit: {logit:.2f})', va='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# import pickle\n\n# # Save\n# with open('logit_lens_results.pkl', 'wb') as f:\n#     pickle.dump(dict_output, f)\n\n# # Load later\n# with open('logit_lens_results.pkl', 'rb') as f:\n#     dict_output = pickle.load(f)\n#     decoded_tokens = dict_output['decoded_tokens']\n#     decoded_logits = dict_output['decoded_logits']\n#     text_tokens = dict_output['text_tokens']"
  },
  {
    "objectID": "notebooks/Branching model tutorial.html",
    "href": "notebooks/Branching model tutorial.html",
    "title": "Recreating branching model animations",
    "section": "",
    "text": "Artem Kirsanov, August 2023\n\nimport numpy as np\nfrom copy import deepcopy\nimport itertools\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom numpy import radians as rad\nfrom matplotlib.animation import FuncAnimation\nfrom scipy.ndimage import convolve,convolve1d\nimport cmasher\nimport seaborn as sns\n\n\nMatplotlib\n\nNUM_LAYERS = 20\nNEURONS_PER_LAYER = 10\n\n\ndef network_init():\n    return np.zeros((NUM_LAYERS, NEURONS_PER_LAYER),dtype=bool)\n\n\ndef network_advance(old_network, sigma,spont_prob):\n    '''Advance one time step'''\n    network = deepcopy(old_network)\n    spont = np.random.rand(*network.shape)\n    network[spont&lt;spont_prob] = 1 # Random spontaneous activity\n    for layer_num in range(NUM_LAYERS-1, 0, -1):\n        # Randomly propagate, starting from the last layer\n        propagation_mask = np.random.rand(NEURONS_PER_LAYER) &lt; sigma*np.sum(network[layer_num-1,:])/NEURONS_PER_LAYER\n        network[layer_num] = propagation_mask\n        network[layer_num-1] = np.zeros(NEURONS_PER_LAYER)\n    return network\n\n\ndef run_simulation(network, n_steps, sigma=1, spont_prob=0.01):\n    '''Run simulation with stochastic activity for n_steps'''\n    network_states = np.zeros((n_steps, NUM_LAYERS, NEURONS_PER_LAYER))\n    network_states[0,:,:] = network\n\n    for step in range(1,n_steps):\n        network_states[step, :,:] = network_advance(network_states[step-1, :,:], sigma,spont_prob)\n    return network_states\n\n\nnetwork = network_init()\nevolution = run_simulation(network, 50, sigma=1, spont_prob=0.01)\n\n\nfig, ax = plt.subplots(1,1,figsize=(10,5),dpi=200)\nax.axis(False)\nfig.set_facecolor(\"white\")\nax.set_facecolor(\"white\")\n\ncmesh = ax.pcolormesh(evolution[0,:,:].T, edgecolors='white', vmin=0, vmax=1,linewidth=2, cmap=plt.cm.coolwarm)\n\ndef anim_function(frame_num):\n    cmesh.set_array(evolution[frame_num,:,:].T)\n    return cmesh,\n\nanim = FuncAnimation(fig, anim_function, frames=np.arange(evolution.shape[0]), interval=30)\nanim.save(\"Network evolution raw fast.mp4\")\n\n[01/29/26 13:38:31] INFO     Animation.save using &lt;class 'matplotlib.animation.FFMpegWriter'&gt;     animation.py:1076\n\n\n\n                    INFO     MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec         animation.py:319\n                             rawvideo -s 2000x1000 -pix_fmt rgba -framerate 33.333333333333336                     \n                             -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y 'Network                    \n                             evolution raw fast.mp4'                                                               \n\n\n\n\n\n\n\n\n\n\n\n# --- Running the model\nnetwork = network_init()\nevolution = run_simulation(network, 500, sigma=1, spont_prob=0.01)\n\n# --- Smoothing activity\ndef smooth_activity(network_states, time_stretch=3):\n    '''\n        Smooth the activity in time for a more eye-pleasant animation\n    '''\n    def get_symmetric_kernel(slope=-20, npoints=100):\n        t = np.linspace(0,1,npoints)\n        kernel = np.zeros_like(t)\n        t_mask = t&gt;0.5\n        kernel[t_mask]=np.exp(slope*t[t_mask])\n        kernel[(t&lt;=0.5)]=np.exp(slope*t[t_mask])[::-1]\n        return kernel/kernel[t_mask][0]\n\n    kernel = get_symmetric_kernel(-60)\n    smoothed_activity = np.zeros((network_states.shape[0]*time_stretch, network_states.shape[1], network_states.shape[2]))\n    smoothed_activity[::time_stretch, :, :] = network_states\n    smoothed_activity = convolve1d(smoothed_activity, kernel, axis=0,mode=\"constant\",origin=0)\n    return smoothed_activity\n\n\n# --- Animation\nsmoothed_evolution = smooth_activity(evolution)\nfig, ax = plt.subplots(1,1,figsize=(10,5),dpi=200)\nax.axis(False)\nfig.set_facecolor(\"white\")\nax.set_facecolor(\"white\")\n\ncmap = cmasher.get_sub_cmap(sns.color_palette(\"mako\",as_cmap=True),0.2,1)\ncmesh = ax.pcolormesh(smoothed_evolution[0,:,:].T, edgecolors='white', vmin=0, vmax=1,linewidth=2, cmap=cmap)\n\ndef anim_function(frame_num):\n    cmesh.set_array(smoothed_evolution[frame_num,:,:].T)\n    return cmesh,\n\nanim = FuncAnimation(fig, anim_function, frames=np.arange(smoothed_evolution.shape[0]), interval=30)\nanim.save(\"Network evolution smoothed.mp4\")\n\n[01/29/26 13:38:53] INFO     Animation.save using &lt;class 'matplotlib.animation.FFMpegWriter'&gt;     animation.py:1076\n\n\n\n                    INFO     MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec         animation.py:319\n                             rawvideo -s 2000x1000 -pix_fmt rgba -framerate 33.333333333333336                     \n                             -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y 'Network                    \n                             evolution smoothed.mp4'                                                               \n\n\n\n\n\n\n\n\n\n\n\n\nManim\n\nNUM_LAYERS = 10\nNEURONS_PER_LAYER = 10\nNUM_FRAMES=2000\n\n# --- Simulation\nnetwork = network_init()\nnetwork_states = run_simulation(network, NUM_FRAMES, sigma=1,spont_prob=0.01)\nsmoothed_states = smooth_activity(network_states)\n\n\ndef multilayered_graph(subset_sizes, edge_prob=0.35):\n    ''' Generate a networkx multilayered graph with specied layer sizes '''\n    extents = nx.utils.pairwise(itertools.accumulate([0] + subset_sizes))\n    layers = [range(start, end) for start, end in extents]\n    G = nx.Graph()\n    for (i, layer) in enumerate(layers):\n        G.add_nodes_from(layer, layer=i)\n    for layer1, layer2 in nx.utils.pairwise(layers):\n        all_edges = list(itertools.product(layer1, layer2))\n        selected_edges = np.random.choice(range(len(all_edges)),  size=int(len(all_edges)*edge_prob), replace=False)\n        for k in selected_edges:\n            G.add_edge(*all_edges[k])\n    return G\n\n\nfrom manim import *\nimport networkx as nx\nfrom scipy.interpolate import interp1d\nimport itertools\n\n\n# --- Animation with Manim\nclass BranchingModelRearranging(Scene):\n    def construct(self):\n\n        # Set up coordinate systems\n        shuffled_ax = Axes(x_range=(0,NUM_LAYERS), y_range=(0,NEURONS_PER_LAYER),x_length=7, y_length=7)\n        layers_ax = Axes(x_range=(0,NUM_LAYERS), y_range=(0,NEURONS_PER_LAYER),x_length=13, y_length=7)\n\n        # --- Mapping\n        mapping = np.array(list(itertools.product(range(shuffled_ax.x_range[1]), range(shuffled_ax.y_range[1]))), dtype=object)\n        layout_layered = {k: layers_ax.c2p(*mapping[k]) for k in range(NUM_LAYERS*NEURONS_PER_LAYER)}\n        np.random.shuffle(mapping)\n        layout_shuffle = {k: shuffled_ax.c2p(*mapping[k]) for k in range(NUM_LAYERS*NEURONS_PER_LAYER)}\n\n        # Construct a graph object\n        G = multilayered_graph(([NEURONS_PER_LAYER]*NUM_LAYERS))\n        graph = Graph.from_networkx(G,layout=layout_shuffle,vertex_config={'radius': 0.2},\n                                    edge_config={\"stroke_width\":0.5, \"stroke_color\":GRAY})\n\n        # Interpolation function to animate the color of the nodes according to simulation data\n        value_interp_function = interp1d(np.arange(smoothed_states.shape[0]),\n                                         smoothed_states.reshape(smoothed_states.shape[0], NUM_LAYERS*NEURONS_PER_LAYER), axis=0)\n\n        cmap = cmasher.get_sub_cmap(sns.color_palette(\"mako\",as_cmap=True),0.2,1)\n\n\n        def update_node_colors(graph):\n            for k in range(len(G.nodes)):\n                color =  cmap(value_interp_function(time_tracker.get_value())[k])\n                graph[k].set_color(rgba_to_color(color))\n\n        time_tracker = ValueTracker() # Progressing through simulation data\n        graph.add_updater(update_node_colors)\n        self.add(graph)\n\n        # --- Animating (make sure that there is enough frames in the simulation data)\n        FPS = 30\n        PLAY_TIME_BEFORE_REARRANGING = 20\n        PLAY_TIME_AFTER_REARRANGING = 5\n        REARRANGING_TIME = 2\n\n        def get_shuffle2layered_anims():\n            return [graph[k].animate.move_to(layout_layered[k]) for k in range(len(G.nodes))]\n\n        def animate_network(playing_time):\n            self.play(time_tracker.animate.increment_value(int(playing_time*FPS)), run_time=playing_time, rate_func=linear)\n\n        animate_network(PLAY_TIME_BEFORE_REARRANGING)\n\n\n        self.play(*(get_shuffle2layered_anims() +\n                    [time_tracker.animate.increment_value(int(REARRANGING_TIME*FPS))]),\n                    run_time=REARRANGING_TIME, rate_func=linear)\n\n        animate_network(PLAY_TIME_AFTER_REARRANGING)\n        self.wait()"
  }
]